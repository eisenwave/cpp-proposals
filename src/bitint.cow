\cowel_include{libwg21.cow}

\wg21_head(
  title = Bit-precise integers
){
\dl{
  \dt{Document number:} \dd{\docnum{P3666R2}}
  \dt{Date:}            \dd{\tt{2025-11-13}}
  \dt{Audience:}        \dd{EWG, LEWG}
  \dt{Project:}         \dd{ISO/IEC 14882 Programming Languages — C++, ISO/IEC JTC1/SC22/WG21}
  \dt{Reply-to:}        \dd{Jan Schultke <\mail{janschultke@gmail.com}>}
  \dt{GitHub Issue:}    \dd{\ref(https://wg21.link/P3666/github)}
  \dt{Source:}          \dd{\ref(https://github.com/Eisenwave/cpp-proposals/blob/master/src/bitint.cow)}
}
\hr
}

\Babstract{
C23 has introduced so-called "bit-precise integers" into the language,
which should be brought to C++ for compatibility, among other reasons.
Following an exploration of possible designs in \ref(P3639R0) "The \tt{_BitInt} Debate",
this proposal introduces a new set of fundamental types to C++.
}

\h2(listed=false){Contents}

\make_contents

\h2{Revision history}

\h3{Changes since R1}

\ul{
  \li{expanded \ref(#passing-bitint) with an observation about return types}
  \li{fixed missing return types on \tcode{abs} throughout the paper}
}

\h3{Changes since R0}

\ul{
  \li{
    updated \ref(#bit-int-1) following the publication of \ref(N3699)
  }
  \li{
    added \ref(#make_signed-and-make_unsigned)
    and corresponding wording changes in \ref(#meta.trans.sign)
  }
  \li{
    permitted bit-precise integers as underlying types of enumerations
    as proposed in \ref(N3705);
    see \ref(#underlying-bitint) and \ref(#dcl.enum)
  }
  \li{
    further changed \ref(#conv.prom) wording,
    taking promotion of enumerations with underlying bit-precise integer type into account
  }
  \li{
    added \ref(#diff.lex) Annex C entry for the difference in the type of \tcode{0wb}
  }
  \li{
    various minor wording tweaks and added notes
  }
  \li{
    converted green notes into aqua "editor's notes" to more more clearly
    distinguish them from wording changes
  }
}

\h2{Introduction}

In distant history,
there have been various attempts at standardizing multi-precision integers in C++,
such as
\ref(N1692) "A Proposal to add the Infinite Precision Integer to the C++ Standard Library",
\ref(N1744) "Big Integer Library Proposal for C++0x", and
\ref(N4038) "Proposal for Unbounded-Precision Integer Types",
all of which have been abandoned by the authors.
However, there has always been some enthusiasm in the committee for such a feature.

I am picking up where they have left off.
Whether this results in a C++ feature or adds corpses
to the graveyard of multi-precision papers remains to be seen.

\h3{C23}

Recently, WG14's
\ref(N2763) introduced the \tcode{_BitInt} set of types to the C23 standard,
and \ref(N2775) further enhanced this feature with literal suffixes.
For example, this feature may be used as follows:

\cppblock{
// 8-bit unsigned integer initialized with value 255.
// The literal suffix wb is unnecessary in this case.
unsigned _BitInt(8) x = 0xFFwb;
}

In short, the behavior of these \dfn{bit-precise integers} is as follows:

\ul{
  \li{
    No integer promotion to \tcode{int} takes place.
  }
  \li{
    Mixed-signedness comparisons, implicit conversions,
    and other permissive feature are supported.
  }
  \li{
    They have lower conversion rank than standard integers,
    so an operation between \tcode{_BitInt(8)} and \tcode{int} yields \tcode{int},
    as does an operation with \tcode{_BitInt(N)} where \tcode{N} is the width of \tcode{int}.
    They only have greater conversion rank when their width is greater.
  }
  \li{
    Widths up to \tcode{BITINT_MAXWIDTH} are allowed,
    with padding bits being added if needed.
    \tcode{BITINT_MAXWIDTH} is at least \tcode{64}.
  }
}

\h3{P3140R0 "\tt{std::int_least128_t}"}

In parallel,
I proposed \ref(P3140R0) which would add 128-bit integers as \tcode{std::int_least128_t}
to the C++ standard.
It became apparent to me that standardizing just a single width of 128
and not solving the \tcode{_BitInt} C compatibility problem would be futile,
so I've stepped away from the proposal.
However, the feedback and experience gained from P3140
made it well worth the time spent.

\h3{P3639R0 "The \tt{_BitInt} Debate"}

I've subsequently proposed \ref(P3639R0) "The \tt{_BitInt} Debate",
which shifts the goal to compatibility with C's \tcode{_BitInt} type,
and attempts to answer whether the set of types corresponding to \tcode{_BitInt}
should be a class template or a family of fundamental types.
P3639R0 received much feedback in 2025.
First, from SG22:

\Bquote{
The WG14 delegation to SG22 believes
that the C++ type family that deliberately corresponds to _BitInt
(perhaps via compatibility macros) should be... (Fundamental/Library)

\table(class=five-way-poll){
  \tr{\th{SF}\th{F}\th{N}\th{L}\th{SL}}
  \tr{\td{8}\td{1}\td{1}\td{0}\td{0}}
}

WG21
\table(class=five-way-poll){
  \tr{\th{SF}\th{F}\th{N}\th{L}\th{SL}}
  \tr{\td{4}\td{5}\td{0}\td{0}\td{0}}
}
}

The overall sentiment in SG22 was that a fundamental type is "inevitable".
This is reflected in the polls.
SG6 also saw the paper, but had no clear opinion on the fundamental/library problem.
Last but not least, EWG also saw the paper in Sofia 2025,
with the following two polls:

\Bquote{
P3639R0: EWG prefers that _BitInt-like type be a FUNDAMENTAL TYPE (in some form) in C++.
\five_way_poll(13,9,9,5,4)

Result: consensus

P3639R0: EWG prefers that _BitInt-like type be a LIBRARY TYPE (in some form) in C++.
\five_way_poll(8,9,14,8,3)

Result: not consensus
}

While the general direction for the paper is clear (fundamental type),
there are many contentious design issues,
such as the minimum supported \tt{BITINT_WIDTH}
or how permissive bit-precise integers should be with implicit conversions.
You can identify the most contentious problems as blocks such as:

\Bdecision{
This is an example.
}

\h2{Motivation}

\h3{Computation beyond 64 bits}

Computation beyond 64-bit bits, such as with 128-bits is immensely useful.
A large amount of motivation for 128-bit computation can be found in \ref(P3140R0).
Computations in cryptography, such as for RSA require even 4096-bit integers.

Even when performing most operations using 64-bit integers,
there are certain use cases where temporarily, twice the width is needed.
For example, the implementation of \tcode{linear_congruential_engine<uint64_t>}
requires the user of 128-bit arithmetic,
as does arithmetic with 64-bit
\ref(https://en.wikipedia.org/wiki/Fixed-point_arithmetic){fixed-point numbers (e.g. \tt{Q32.32})}.

\h3{Cornerstone of standard library facilities}

There are various existing and possible feature
library facilities that would greatly benefit from an N-bit integer type:

\ul{
  \li{
    As mentioned above,
    the implementation of \tcode{linear_congruential_engine<uint64_t>}
    requires the use of 128-bit integers.
  }
  \li{
    \tcode{bitset} has constructors taking \tcode{unsigned long long} and a
    \tcode{to_ullong} member function that converts from/to integers.
    This is clunky and limited considering that bitsets can be much larger
    than \tcode{unsigned long long}.
    Bit-precise integers would be a superior alternative to \tcode{unsigned long long} here.
  }
  \li{
    \ref(P3161R4) proposes library features such as \tcode{add_carry}
    or \tcode{mul_wide} which produce a wider integer result than the operands.
    For example:
    \cppblock{
template<class T>
struct mul_wide_result {
  T low_bits;
  T high_bits;
};
template<class T>
  constexpr mul_wide_result<T> mul_wide(T x, T y) noexcept;
}
    Proposals like these are arguably obsolete if the same operation can be
    expressed by simply casting the operands to an integer with double the width
    prior to the multiplication.
  }
}

\h3{C ABI compatibility}

C++ currently has no portable way to call C functions such as:

\cppblock{
_BitInt(32)  plus( _BitInt(32) x,  _BitInt(32) y);
_BitInt(128) plus(_BitInt(128) x, _BitInt(128) y);
}

While one could rely on the ABI of \tcode{uint32_t} and \tcode{_BitInt(32)}
to be identical in the first overload,
there certainly is no way to portably invoke the second overload.

This compatibility problem is not a hypothetical concern either; it is an urgent problem.
There are already targets with \tcode{_BitInt} supported by major compilers,
and used by C developers:

\style{
  .center {
    margin-left: auto;
    margin-right: auto;
  }
}

\table(class=center){
  \tr{
    \th{Compiler}
    \th{\tt{BITINT_MAXWIDTH}}
    \th{Targets}
    \th{Languages}
  }
  \tr{
    \td{clang 16+}
    \td{\tcode{8'388'608}}
    \td{all}
    \td{C & C++}
  }
  \tr{
    \td{GCC 14+}
    \td{\tcode{65'535}}
    \td{64-bit only}
    \td{C}
  }
  \tr{
    \td{MSVC}
    \td{\N{CROSS MARK}}
    \td{\N{CROSS MARK}}
    \td{\N{CROSS MARK}}
  }
}

\h3{Resolving issues with the current integer type system}

\tcode{_BitInt} as standardized in C solves multiple issues that
the standard integers (\tcode{int} etc.) have.
Among other problems,
integer promotion can result in unexpected signedness changes.

\Bex{
The following code has undefined behavior
if \tcode{int} is a 32-bit signed integer (which it is on many platforms).

\cppblock{
uint16_t x = 65'535;
uint16_t y = x * x;
}

During the multiplication \tcode{x * x},
\tcode{x} is promoted to \tcode{int},
and the result of the multiplication \tcode{4'294'836'225}
is not representable as a 32-bit signed integer.
Therefore, signed integer overflow takes places \N{HORIZONTAL ELLIPSIS} given unsigned operands.
}

\Bex{
The following code may have surprising effects
if \tcode{std::uint8_t} is an alias for \tcode{unsigned char}
and gets promoted to \tcode{int}.

\cppblock{
std::uint8_t x = 0b1111'0000;
std::uint8_t y = ~x >> 1; // y = 0b1000'01111
}

Surprisingly, \tcode{y} is not \tcode{0b111}
because \tcode{x} is promoted to \tcode{int} in \tcode{~x},
so the subsequent right-shift by \tcode{1} shifts one set bit into \tcode{y} from the left.
Even more surprisingly, if we had used \tcode{auto} instead of \tcode{std::uint8_t} for \tcode{y},
\tcode{y} would be \tcode{-121},
despite our code seemingly using only unsigned integers.
}

Overall, the current integer promotion semantics are extremely surprising
and make it hard to write correct code involving promotable unsigned integers.
Promotion also makes it hard to expose small integers (e.g. 10-bit unsigned integer)
that exist in hardware (e.g. FPGA) in the language,
since all operations would be performed using \tcode{int}.
Unconventional hardware such as FPGAs are pillar of the motivation for \tcode{_BitInt}
laid out in \ref(N2763).

\h3{Portable exact-width integers}

There is no portable way to use an integer with exactly 32 bits in standard C++.
\tcode{int_least32_t} and \tcode{long} may be wider,
and \tcode{int32_t} is an optional type alias
which only exists if such an integer type has no padding bits.
Having additional non-padding bits may be undesirable when implementing serialization,
networking, etc. where the underlying file format or network protocol is specified
using exact widths.

While most platforms support 32-bit integers as \tcode{int32_t},
their optionality is a problem for use in the standard library and other ultra-portable libraries.
There are many use cases where padding bits would be an acceptable sacrifice
in exchange for writing portable code,
and bit-precise integers fill that gap in the language.

\h2{Core design}

The overall design strategy is as follows:

\ul{
  \li{
    The proposal is a C compatibility proposal first and foremost.
    Whenever possible, we match the behavior of the C type.
  }
  \li{
    The goal is to deliver a minimal viable product (MVP)
    which can be integrated into the standard as quickly as possible.
    This gives us plenty of time to add standard library support wherever desirable over time,
    as well as other convenience features surrounding \tcode{_BitInt}.
  }
}

\h3{Why not a class template?}

\ref(P3639R0) explored in detail whether to make it a fundamental type or a library type.
Furthermore, feedback given by SG22 and EWG was to make it a fundamental type, not a library type.
This boils down to two plausible designs
(assuming \tcode{_BitInt} is already supported by the compiler), shown below.

\style{
  .toc h4 {
    margin-left: 1.5em;
  }
  .toc h5 {
    margin-left: 3em;
  }
  .toc h6 {
    margin-left: 4.5em;
  }

  #tony-table {
    margin-left: auto;
    margin-right: auto;
    width: 90%;
    table-layout: fixed;
  }
  #tony-table td {
    background-color: var(--deep-background-color);
    width: 50%;
  }
}

\table(id=tony-table){
\tr{
    \th{\N{MATHEMATICAL DOUBLE-STRUCK CAPITAL F} \N{EN DASH} Fundamental type}
    \th{\N{MATHEMATICAL DOUBLE-STRUCK CAPITAL L} \N{EN DASH} Library type}
}
\tr{
\td{\codeblock(cpp, borders=false){
template <size_t N>
using bit_int =
    _BitInt(N);




template <size_t N>
using bit_uint =
    unsigned _BitInt(N);
}}
\td{\codeblock(cpp, borders=false){
template <size_t N>
class bit_int {
  private:
    _BitInt(N) _M_value;
  public:
    // ...
};
template <size_t N>
class bit_uint
  { /* ... */; };
}}
}
}

The reasons why we should prefer the left side are described in the following subsections.

\h4{Full C compatibility requires fundamental types}

\tcode{_BitInt} in C can be used as the type of a bit-field, among other places:

\cppblock{
struct S {
  // 1. _BitInt as the underlying type of a bit-field
  _BitInt(32) x : 10;
};

// 2. _BitInt in a switch statement
_BitInt(32) x = 10;
switch (x) {}

// 3. _BitInt used as a null pointer constant
void* p = 0wb;

// 4. _BitInt used as underlying type of enumeration
//    (NOT valid now, but may be in the future)
enum S : _BitInt(32) { X = 0 };
}

Since C++ does not support the use of class types in bit-fields,
such a \tcode{struct S} could not be passed from C++ to a C API.
A developer would face \em{severe} difficulties
when porting C code which makes use of these capabilities to C++
and if bit-precise integers were a class type in C++.

\h4{Common spelling of \tt{unsigned _BitInt(N)}}

If bit-precise integers were class types in C++,
this would cause a serious problem with a common spelling
that can be used in both C and C++ headers,
even if there was a \tcode{\hl(macro){_BitInt}} compatibility macro.

\cppblock{
#define _BitInt(...) std::bit_int<__VA_ARGS__>

unsigned \hl(macro){_BitInt}(8) x; // error: cannot combine 'unsigned' with class type
}

There are some workarounds to the problem,
but they all seem unattractive:

\ul{
  \li{
    Permitting \tcode{signed} and \tcode{unsigned} to be combined with class types
    in general,
    perhaps with the effect of applying \tcode{std::make_signed} and \tcode{std::make_unsigned}.
    This would lead to a bifurcation of the language where both a builtin feature
    and a type trait achieve the same effect.
  }
  \li{
    "Blessing" the \tcode{std::bit_int<N>} \grammarterm{type-name}
    so it can be combined with \tcode{unsigned}.
    This would be a highly unusual special case in the language.
  }
  \li{
    Making \tcode{\hl(macro){_BitInt(...)}} expand to an unspecified construct
    that can be combined with \tcode{signed} and \tcode{unsigned}.
    This means there needs to be a fundamental type,
    although that fundamental type only acts as a proxy for the \tcode{std::bit_int} class type.
    Once again, this comes off as an unusual special case.
  }
  \li{
    Introducing a \tcode{\hl(macro){_BitUint(...)}} macro for unsigned bit-precise integers,
    and insisting that both C and C++ developers use this for interoperability.
    This feels like an unnecessary burden for C developers
    considering that their spelling works perfectly fine
    and that we have other design options which keep C code intact.
  }
}

\h4{C compatibility would require an enormous amount of operator overloads etc.}

Integer types can be used in a large number of places within the language.
If we wanted a \tcode{std::bit_int} class type to be used in the same places
(which would be beneficial for C-interoperable code),
we would have to add a significant amount of operator overloads
and user-defined conversion functions:

\ul{
  \li{There are conversion to/from floating-point types and other integral types.}
  \li{There are conversion to/from enumeration types.}
  \li{
    There are conversion to/from pointers,
    at least for \tcode{_BitInt}s of the same width as \tcode{uintptr_t}.
  }
  \li{
    Integers can be used to add offsets onto pointers, and by proxy,
    in the subscript operator of builtin arrays.
  }
  \li{
    Arithmetic operators can be used to operate between any mixture of arithmetic types,
    such as \tcode{_BitInt(32) + float}.
  }
}

Any discrepancies would lead to some code using bit-precise integers behaving
differently in C and C++, which is undesirable.

Furthermore, the \tcode{wb} \grammarterm{integer-suffix} for \tcode{_BitInt}
is fairly complicated to implement as a library feature
because the resulting type depends on the numeric value of the literal.
This means it would presumably be implemented like:

\: manual highlights are workaround for https://github.com/eisenwave/ulight/issues/109
\cppblock{
template<char... Chars>
  constexpr auto \hl(keyword){operator}""wb() { /* ... */ }
template<char... Chars>
  constexpr auto \hl(keyword){operator}""WB() { /* ... */ }
template<char... Chars>
  constexpr auto \hl(keyword){operator}""uwb() { /* ... */ }
template<char... Chars>
  constexpr auto \hl(keyword){operator}""UWB() { /* ... */ }
template<char... Chars>
  constexpr auto \hl(keyword){operator}""uWB() { /* ... */ }
template<char... Chars>
  constexpr auto \hl(keyword){operator}""Uwb() { /* ... */ }
}

Seeing that properly emulating C's behavior for \tcode{_BitInt} (and its suffixes)
requires a mountain of complicated operator overload sets,
user-defined conversion functions,
converting constructors, and
user-defined literals,
it seems unreasonable to go this direction.

A major selling point of a library type is that library types have more teachable interfaces,
since the user simply needs to look at the declared members of the class
to understand how it works.
If the interface is a record-breaking convoluted mess,
this benefit is lost.
If we choose not to add all this functionality,
then we lose a large portion of C compatibility.
Either option is bad, and making \tcode{std::bit_int} a fundamental type
seems like the only way out.

\h4{Constructors cannot signal narrowing}

Some C++ users prefer list initialization because it prevents narrowing conversion.
This can prevent some mistakes/questionable code:

\cppblock{
unsigned x = -1; // OK, x = UINT_MAX, but this looks weird
unsigned y{ -1 };  // error: narrowing conversion
}

This would not be feasible if \tcode{std::bit_int} was a library type
because narrowing cannot be signaled by constructors.
Consider that \tcode{std::bit_int} and \tcode{std::bit_uint}
should have a non-explicit constructor (template)
accepting \tcode{int} (and other integral types) to enable compatibility in situations like:

\cppblock{
#ifdef __cplusplus
typedef std::bit_uint<32> u32; // C++
#else
typedef unsigned _BitInt(32) u32; // C
#endif
// Common C and C++ code, possibly in a header:

// OK, converting int → u32.
// Using "incorrectly typed" zeros is fairly common, both in C and in C++.
u32 x = 0;
// OK, same conversion, but would be considered narrowing in C++.
// Not very likely to be written.
u32 y = -1;
}

If such a \tcode{std::bit_uint<32>(int)} constructor existed,
the following C++ code would not raise any errors:

\cppblock{
std::bit_uint<32> x{ 0 };  // OK, as expected
std::bit_uint<32> y{ -1 }; // OK?! But this looks narrowing!
}

This code simply calls a \tcode{std::bit_uint<32>(int)} constructor,
and while the initialization of \tcode{y} is \em{spiritually} narrowing,
no narrowing conversion actually takes place.
In conclusion,
if \tcode{std::bit_int} was a library type,
C++ users who use this style would lose what they consider a valuable safety guarantee.

\editnote{
It can be argued that using list-initialization for this purpose is an anti-pattern
and only solves a subset of the issues that compiler warnings and linter warnings should address.
Personally, I have no strong position on this issue.
}

\h4(id=tiny-integers){Tiny integers are useful in C++}

In some cases, tiny \tcode{bit_int}'s may be useful as the underlying type of an enumeration:

\cppblock{
enum struct Direction : bit_int<2> {
  north, east, south, west,
};
}

By using \tcode{bit_int<2>} rather than \tcode{unsigned char},
every possible value has an enumerator.
If we used e.g. \tcode{unsigned char} instead,
there would be 252 other possible values that simply have no name,
and this may be detrimental to compiler optimization of \tcode{switch} statements etc.

\editnote{
See also \ref(#underlying-bitint).
}

\h4{Special deduction rules}

While this proposal focuses on the minimal viable product (MVP),
a possible future extension would be new deduction rules allowing the following code:

\cppblock{
template <size_t N>
  void f(bit_int<N> x);

f(int32_t(0)); // calls f<32>
}

Being able to make such a call to \tcode{f} is immensely useful because it would allow
for defining a single function template which may be called with every possible
signed integer type,
while only producing a single template instantiation
for \tcode{int}, \tcode{long}, and \tcode{_BitInt(32)},
as long as those three have the same width.
The prospect of being able to write bit manipulation utilities that simply accept
\tcode{bit_uint<N>} is quite appealing.

If \tcode{bit_int<N>} was a class type,
this would not work because template argument deduction would fail,
even if there existed an implicit conversion sequence from \tcode{int32_t}
to \tcode{bit_int<32>}.
These kinds of deduction rules may be shutting the door on this mechanism forever.

\h4{Special overload resolution rankings}

Yet another possible future extension would be rankings for overload resolution
that take integer width into account.

\Bex{
Special overload rankings could make bit-precise integers
more easily interoperate with existing overload sets:
\cppblock{
struct QString { // see Qt 6 documentation
  static QString number(int n, int base = 10);
  static QString number(long n, int base = 10);
  static QString number(long long n, int base = 10);
  // ...
};

QString::number(0wb); // currently ambiguous, but could call QString::number(int)
}

This could be valid if \tcode{number(int)} was considered a better match
on the basis that its width is closer to that of \tcode{0wb}.
Further disambiguation could be applied if \tcode{int} and \tcode{long} had the same width.
}

\Bex{
Special overload rankings could make it possible to create non-template overload sets
that cover a greater range of widths:
\cppblock{
bit_uint<64>  clmul_wide(bit_uint<32>);
bit_uint<128> clmul_wide(bit_uint<64>);

clmul_wide(128wb); // OK, calls clmul_wide(bit_uint<32>)
}
}

These overload ranking rules would be difficult or impossible to define using a class type.
Of course, they are not proposed, and it's not certain whether such rules are desirable to have,
but it would be unfortunate to shut the door on these possible features forever.

\h4{Quality of implementation requires a fundamental type}

While a library type \tcode{class bit_int} gives the implementation
the option to provide no builtin support for bit-precise integers,
to achieve high-quality codegen,
a fundamental type is \em{inevitably} needed anyway.

\Bex{
When an integer division has a constant divisor, like \tcode{x / 10},
it can be optimized to a fixed-point multiplication,
which is much cheaper:

\cppblock{
unsigned div10(unsigned x) {
    return x / 10;
}
}

For this operation, Clang emits the following assembly:

\codeblock(asm){
\hl(id-label){div10(unsigned int)}:
        mov     ecx, edi
        mov     eax, 3435973837
        imul    rax, rcx
        shr     rax, 35
        ret
}

Basically, the result is rewritten as \tcode{x * 3435973837ull >> 35}.
This optimization is called \dfn{strength reduction}
and may lead to dramatically faster code,
especially when the hardware has no direct support for integer division.
Similarly, multiplication can be strength-reduced to bit-shifting
when a factor is a power of two,
remainder operations can be reduced to bitwise AND when the divisor is a power of two, etc.

Performing strength reduction requires the compiler to be aware that a division is taking place,
and this fact is lost when division is implemented in software,
as a loop which expands to hundreds of IR instructions when unrolled.
}

Furthermore, the compiler frontend needs to understand certain operations
to warn about obvious mistakes such as division by zero,
shifting by an overly large amount,
producing signed integer overflow unconditionally, etc.
Use of \tcode{pre} on e.g. \tcode{bit_int::operator/} cannot be used to achieve this
because numerics code needs to have no hardened preconditions and no contracts,
for performance reasons.

Last but no least,
a fundamental type is needed to speed up constant evaluation.
Something like integer division between two \tcode{bit_int<128>}
may be much faster as a compiler-builtin operation
compared to constant-evaluating a "software division" loop with 128 iterations
necessary to implement \ref(https://en.wikipedia.org/wiki/Binary_number#Division){binary division}.

If we accept the premise that a fundamental type is needed anyway
(possibly as an implementation detail of a class template),
then the class template actively harmful bloat:
\ul{
  \li{
    Any arithmetic operation needs to go through overload resolution,
    competing with countless other \tcode{operator+}s
    (there are many in the standard library already).
    Even if implementers special-case these operations to circumvent the
    (usually awful) diagnostic quality of a failed call to \tcode{operator+},
    there remains substantial cost:
    overload resolution is expensive.
  }
  \li{
    Every distinct \tcode{bit_int<N>} and \tcode{bit_uint<N>} would be a separate instantiation
    of a relatively large class template,
    which would undoubtedly add compilation cost.
  }
  \li{
    Invocations of member functions or operator overloads
    may add cost to debug builds and constant evaluation.
  }
}


\h3{Why the \tt{_BitInt} keyword spelling?}

I also propose to standardize the keyword spelling
\tcode{_BitInt} and \tcode{unsigned _BitInt}.
I consider this to a "C compatibility spelling" rather than the preferred one
which is taught to C++ developers.
See also \ref(#teaching-principles).

While a similar approach could be taken
as with the \tcode{\hl(macro){_Atomic}} compatibility macro,
macros cannot be exported from modules,
and macros needlessly complicate the problem compared to a keyword.
Furthermore, to enable compiling shared C and C++ headers, all of the spellings
\tcode{_BitInt}, \tcode{signed _BitInt} and \tcode{unsigned _BitInt} need to be valid.
This goes far beyond the capabilities that a compatibility macro like \tcode{\hl(macro){_Atomic}}
can provide without language support.
If the \tcode{\hl(macro){_BitInt(...)}} macro simply expanded to \tcode{bit_int<__VA_ARGS__>},
this may result in the ill-formed code \tcode{signed bit_int<N>}.

The most plausible fix would be to create an exposition-only \tcode{\exposid{bit-int}}
spelling to enable \tcode{signed \exposid{bit-int<N>}},
which makes our users beg the question:

\Bquote{
Why is there a compatibility macro for an exposition-only keyword spelling?!
Why are we making everything more complicated by not just copying the keyword from C?!
Why is this exposition-only when it's clearly useful for users to spell?!
}

The objections to a keyword spelling are that it's not \em{really} necessary,
or that it "bifurcates" the language by having two spellings for the same thing,
or that those ugly C keywords should not exist in C++.
Ultimately, it's not the job of WG21 to police code style;
both spellings have a right to exist:

\ul{
  \li{
    The \tcode{bit_int} alias template fits in aesthetically with the rest of the language,
    and conveys clearly (via "pointy brackets") that the given width is a constant expression.
  }
  \li{
    The \tcode{_BitInt} spelling is useful for writing C/C++-interoperable code,
    and C compatibility is an important design goal.
  }
}

It seems like both spellings are going to exist,
whether \tcode{_BitInt} is a keyword or compatibility macro.
Since there is no clear technical benefit to a macro,
the keyword is the \em{only} logical choice.

\Bnote{
Clang already supports the \tcode{_BitInt} keyword spelling as a compiler extensions,
so this is standardizing existing practice.
}

\h3(id=underlying-bitint){Underlying type of enumerations}

The following C code is not valid C23,
but may become valid if \ref(N3705) is accepted.

\cppblock{
// error: '_BitInt(32)' is an invalid underlying type
enum E : _BitInt(32) { x = 0 };
}

There is no obvious reason why \tcode{_BitInt} must not be a valid underlying type,
neither in C nor in C++.
For C++, it seems better to simply allow bit-precise integers in this context
because it is useful; see \ref(#tiny-integers).

Also note that as proposed in \ref(N3705),
bit-precise integers should only be the underlying types of enumerations
when the user explicitly specifies this with \tcode{: _BitInt(N)}:

\cppblock{
enum class E : _BitInt(1024) {
  X = 0x1'0000'0000'0000'0000'0000'0000'0000'0000wb // OK
};

enum E {
  X = 0x1'0000'0000'0000'0000'0000'0000'0000'0000wb // error (most likely)
};
}

As proposed in \ref(N3705) and as in the case of bit-precise bit-fields,
integer promotion should not take place for enumerations
whose underlying type is bit-precise.
If the implementation-defined underlying type of enumerations
could be chosen to be bit-precise,
this would make it implementation-defined whether integer promotion takes place,
by proxy.
It would also be a compatibility pitfall;
C requires bit-precise underlying types to be specified explicitly,
so any choice the implementation makes could interfere with future standardization.

\Bnote{
See \ref(N3550) §6.7.3.3 "Enumeration specifiers" for current restrictions.
Note that in C, "enumerated types" are also classified as "integer types",
unlike in C++.
}

\h3{Should bit-precise integers be optional?}

As in C, \tcode{_BitInt(N)} is only required to support \tcode{N}
of at least \tcode{LLONG_WIDTH}, which has a minimum of \tcode{64}.
This makes \tcode{_BitInt} a semi-optional feature,
and it is reasonable to mandate its existence, even in freestanding platforms.

Of course, this has the catch that \tcode{_BitInt} may be completely useless
for tasks like 128-bit computation.
As unfortunate as that is, the MVP should include no more than C actually mandates.
Mandating a greater minimum width could be done in a future proposal.

\h3(id=bit-int-1){\tt{_BitInt(1)}}

C23 does not permit \tcode{_BitInt(1)} but does permit \tcode{unsigned _BitInt(1)},
mostly for historical reasons
(C did not always requires two's complement representation for signed integers).
This is an irregularity that could make generic programming harder in C++.

However, there are already plans to lift the restriction for C2y;
see \ref(N3699) "Integer Sets, v3".
That proposal makes \tcode{_BitInt(1)} a valid type,
and \tcode{0wb} is changed to be of type \tcode{_BitInt(1)} rather than \tcode{_BitInt(2)}.
That proposal also contains some practical motivation for why
a single-bit should be permitted.

\Bnote{
If \tcode{_BitInt(1)} was allowed,
it would be able to represent the values \tcode{0} and \tcode{-1},
just like an \tcode{int x : 1;} bit-field.
}

\h3{Undefined behavior on signed integer overflow}

\Bdecision{
Whether (and how) to address signed integer overflow in bit-precise integers
specifically is a contentious issue,
which has been discussed in great length.
EWG should decide whether to perpetuate undefined behavior
or to have different behavior for bit-precise integers.
}

I propose to perpetuate bit-precise integers having undefined behavior
on signed integer overflow, just like \tcode{int}, \tcode{long} etc.
This has a few reasons:

\ul{
  \li{
    bit-precise integers have undefined overflow in C,
    so this is what users are used to.
  }
  \li{
    "Solving" signed integer overflow for bit-precise integers is not part of the MVP.
    Undefined behavior can always be defined to do something else,
    so there is no urgent need for this paper to address this issue,
    rather than solving it in a follow-up paper.
  }
  \li{
    Signed integer overflow having undefined behavior is a much broader issue
    that should be looked at in general, for all integer types,
    not just bit-precise integer types.
    Perhaps hardened implementations could have wrapping overflow with erroneous behavior.
    In any case, the problem exceeds the scope of the paper.
  }
  \li{
    It is highly unusual that users would expect signed integer overflow to be well-behaved,
    such as having wrapping behavior.
    Adding two positive numbers and obtaining a negative number is not typically useful.
  }
  \li{
    The undefined behavior here is useful.
    It allows for optimizations such as converting \tcode{x + 3 < 0} into \tcode{x < -3}.
  }
}

That being said,
much of the feedback surrounding bit-precise integers revolved around signed integer overflow.
If we were to make signed integer overflow \em{not} undefined for bit-precise integers,
there are two options that may find consensus:

\ul{
  \li{
    Make signed integer overflow wrapping.
    In other words, most operations would be performed as if by casting to the corresponding
    unsigned type, performing the operation, and casting back.
  }
  \li{
    Make signed integer overflow wrapping \em{and} erroneous.
    This is mirroring Rust's behavior,
    and would typically be implemented by detecting overflow
    on debug builds and in constant evaluation,
    but ignoring it and letting it wrap in release builds.
  }
}

\h3{Permissive implicit conversions}

Just like any other integral type,
the proposal makes bit-precise integers quite permissive when it comes to implicit conversions.
This is disappointing to anyone who wants bit-precise integers
to be a much "stricter" or "safer" alternative to standard integers,
but it is arguably the better design
for various reasons.

\Bdecision{
This is a contentious issue,
and feedback was given that implicit conversions should be limited
for bit-precise integers.
The idea of limiting implicit conversions was also a selling point of \ref(P3639R0).

There are several plausible directions:
\ul{
  \li{
    Perpetuate implicit conversions of the standard integers.
    This is the current design of the proposal;
    see rationale below.
  }
  \li{
    Limit implicit conversions for bit-precise integers.
    For example, converting \tcode{int} to \tcode{unsigned _BitInt(1)} could be invalid.
  }
  \li{
    Perpetuate implicit conversions of standard integers,
    but also add a library wrapper or another \tcode{_BitIntStrict} type family
    with much more restrictive semantics.
  }
}
}

\h4(id=conversions-c-compatibility){C compatibility}

Firstly, the point of perpetuating implicit conversions
is to mirror the C semantics as closely as possible,
which leads to few or no surprises when porting code between the languages,
or when writing C-interoperable headers.

If we look at how C users use \tcode{_BitInt},
\ref(https://github.com/search?q=\N{PERCENT SIGN}22_BitInt\N{PERCENT SIGN}22+language\N{PERCENT SIGN}3AC+&type=code){\
GitHub code search for \tt{"_BitInt" language:C}\
} yields examples such as:

\Bquote{
\cppblock{
// mixing signed and unsigned bit-precise integers
unsigned _BitInt(128) max128s = 0x7FFF'FFFF'FFFF'FFFF'FFFF'FFFF'FFFF'FFFFwb;
// mixing bit-precise and standard integers
unsigned _BitInt(4) a = 1u;
// mixing bit-precise and standard integers of different signedness
unsigned _BitInt(total) bit = 1;
// ... including cases where initialization does not preserve values
unsigned _BitInt(3) max3u = -1;
}
}

If we were to make implicit conversions much more restrictive on the C++ side,
it would become very easy to slip up and accidentally write a header
that does not also compile in C++.

\h4(id=difficult-special-cases){Difficulty of carving out exceptions in the language}

Writing C++ code involving bit-precise integers would be
quite annoying and "flag" many harmless cases
if the rules were too strict.

\Bex{
The following line of code would not compile if converting from \tcode{int}
to \tcode{bit_uint<8>} was unconditionally ill-formed.
\cppblock{
std::bit_uint<8> x = 0; // error?
}

\tcode{0} is "incorrectly signed" for \tcode{std::bit_uint},
and the conversion from \tcode{int} to \tcode{bit_uint<8>} is not value-preserving generally,
but writing code like this is perfectly reasonable.

The workaround would be to use correct literals, such as:
\cppblock{
std::bit_uint<8> x = 0uwb; // OK, conversion bit_uint<1> → bit_uint<8>
}
}

To combat this problem,
it would be necessary to carve out various special cases.
For example, permitting value-preserving conversions with constant expressions
would prevent the example above from being flagged.

\Bnote{
There is precedent for such special casing of value-preserving conversions.
Specifically, see mentions of "narrowing" in
\eelis{dcl.init.list}, \eelis{expr.spaceship}, and \eelis{expr.const}.
}

However, such special cases are insufficient to cover \em{all} harmless cases.

\Bex{
\cppblock{
void for_each_cell(vec3 x) {
  for (int i = 0; i < 3; ++i) {
    do_something(x[i]);
  }
}
}

Even though \tcode{i} is not a constant expression,
\tcode{x[i]} will "just work" no matter what integer type \tcode{vec3::operator[]} accepts.
}

Existing C++ code bases that have not used flags such as \tt{-Wconversion}
from the start are likely filled with many such harmless cases of mixed-sign
implicit conversions.
If bit-precise integer types were introduced into these code bases,
refactoring effort may be unacceptable.

Furthermore, discrepancies between the standard integers and bit-precise integers
would make it much harder to write generic code:

\Bex{
The following function template may be instantiated with any integral type \tcode{T},
but the instantiation would be ill-formed for \tcode{T = unsigned _BitInt(8)}
with restrictive implicit conversions:

\cppblock{
template<std::integral T>
T div_ceil(T x, T y) { // performs integer division, rounding to +inf
  // ⚠️ Could be mixed-sign comparison:
  bool quotient_positive = (x ^ y) >= 0;
  // ⚠️ Could be mixed-sign comparison
  bool adjust = x % y != 0 && quotient_positive;
  // ⚠️ Could be mixed-sign addition between int (0 or 1)
  //    and unsigned _BitInt(N) "x / y":
  // ⚠️ Could be lossy conversion when returning: int → unsigned _BitInt
  return x / y + int(adjust);
}
}

Literally every statement of this template may fail to compile when \tcode{T = unsigned _BitInt(8)},
depending on how strict implicit conversions are.
I conjecture that there are vast amounts of templates like \tcode{div_ceil}.
To accommodate bit-precise integers in this function, a rewrite is necessary:

\cppblock{
template<std::integral T>
T div_ceil(T x, T y) {
\ins{  constexpr auto zero = T(0);}
  bool quotient_positive = (x ^ y) >= \del{0} \ins{zero};
  bool adjust = x % y != \del{0} \ins{zero} && quotient_positive;
  return x / y + \del{int} \ins{T}(adjust);
}
}
}

\Bex{
The following function template involves a mixed-sign operation,
but is entirely harmless for any type \tcode{T}:

\cppblock{
constexpr unsigned mask = 0xf;
T integer = /* ... */;
x &= mask; // equivalent to x = x & mask;
}

Even if \tcode{x} is signed instead of unsigned,
\tcode{x & mask} produces a mathematically identical result.
}

\h4(id=conversions-low-hanging-fruits){Picking some low-hanging fruits}

While conversions between bit-precise integers
and other signed or unsigned integer could be difficult to restrict due to the reasons
above,
other conversions are much more rare
and could more easily be restricted:

\ul{
  \li{Conversions between bit-precise integers and \tcode{bool}.}
  \li{Conversions between bit-precise integers and character types.}
  \li{Conversions between bit-precise integers and floating-point types.}
}

It would be reasonable to ban these conversions unconditionally
because they are likely to be category errors.

\Bug{
Consider the "easter egg" discovered in \ref(https://cplusplus.com/forum/general/105627/):

\Bquote{
I was fixing a couple of minor bugs in a program I've been working on,
when I made the mistake of typing \tcode{cout<<string('\\n', 1);}
instead of \tcode{cout<<string(1,'\\n');}

I didn't get any compile errors and the programs reaction gave me a bit of a laugh.
Instead of the blank line I wanted to put in, I got \tt{:):):):):):):):):):)} (10 of them).
It just made me wonder as a relative C++ beginner what other "easter eggs" are there
that people might feel like sharing.
}

It turns out that \tcode{string('\\n', 1)} is not an "easter egg",
it just results in the Windows terminal displaying a \tcode{char(1)} as "\tt{:)}"
ten times.
The \tcode{string(size_t, char)} overload is called,
and since the \tcode{'\\n'} and \tcode{1} can be converted to \tcode{size_t} and \tcode{char}
without any change in value,
compilers generally don't raise a warning, even with \tcode{-Wconversion enabled}.
}

The least harmful of these conversions is a value-preserving
conversion from a bit-precise integer to a floating-point type.
However, at best, these lack clarity of intent.

\Bex{
Consider a code base with the following two functions computing \math{\msqrt{\mi{x}}}:
\cppblock{
int isqrt(int x);
double sqrt(double x);
}
When the user calls \tcode{sqrt} with an integer operand,
are we sure that this decision was made intentionally?
Is the author unaware that there is a separate function giving the integer results,
or do they actually need the fractional part,
and that is why they called the \tcode{double} overload?
Even if the author wrote \tcode{(int) sqrt(/* ... */)},
this could plausibly be done due to performance considerations.

Similarly, calling \tcode{std::sqrt} with an integer operand could be a major performance bug
on a 32-bit platform with 32-bit \tcode{float} and 64-bit \tcode{double},
considering that this is equivalent to calling \tcode{std::sqrt(double)}.
Perhaps calling \tcode{std::sqrt(float)} was intended.

Conversely, if the author called \tcode{isqrt(10.f)},
the \tcode{float} → \tcode{int} conversion may be value-preserving,
but this call is almost certainly a mistake.
The author likely expected to obtain \tcode{3.1623f}, judging by the operand.
}

\h4(id=conversions-conclusion){Conclusion on implicit conversions}

In conclusion, discrepancies between the standard integers and bit-precise
integers are undesirable;
they introduce a lot of unnecessary problems.
There are many harmless operations like \tcode{T x = 0;} and \tcode{x & mask}
where mixing signedness is okay,
and not every user wants to have warnings, let alone errors for these.
Especially errors would make it hard to write headers that compile both in C and in C++.

The final nail in the coffin is that if the user wants implicit conversions to be restricted,
they have the freedom to add those restrictions via compiler warnings and linter checks.
Having these restrictions standardized in the language robs the user of choice.
If C++26 profiles make progress,
it is likely that C++ will have profiles which restrict implicit conversions,
giving users a standard way to opt into diagnostics.

However, the decision to permit implicit conversions is not set in stone.
Especially the conversions described in \ref(#conversions-low-hanging-fruits)
could be banned for bit-precise integers without much of an issue.

\h3{Raising the \tt{BITINT_MAXWIDTH}}

The proposal currently does not seek to increase the \tcode{BITINT_MAXWIDTH}
beyond what C offers.
That is, \tcode{BITINT_MAXWIDTH} may as low as 64.
I do not consider an increase of the maximum to be part of the MVP.
It's something that can always be done later, if desirable,
without any breaking changes.

\Bdecision{
While the proposal does not propose an increase,
some negative feedback stated that bit-precise integers as a feature
are not worth the standardization effort if they only support a width of \tcode{64}.
Therefore, EWG should decide whether
an increase should take place, and, if so,
whether it should be done within this proposal.
}

It also should be stated that increasing the \tcode{BITINT_MAXWIDTH} is not \em{really}
within the power of WG21 and not even within the power of compiler vendors.

\Bex{
Clang supports a \tcode{BITINT_MAXWIDTH} of up tp \tcode{8'388'608},
but only enables this for certain ABIs.
For example, the \ref(https://gitlab.com/x86-psABIs/x86-64-ABI){x86-64 psABI}
defines an ABI for any bit-precise integer width,
so the full width is available.

However,
the \ref(https://github.com/WebAssembly/tool-conventions/blob/main/BasicCABI.md){"Basic" C ABI for WebAssembly}
(which Clang uses at the time of writing) has the following limitation:

\Bquote{
\tcode{_BitInt(N)} types are supported up to width 128
and are represented as the smallest same-signedness Integer type with at least as many bits.
}

Consequently, \tcode{BITINT_MAXWIDTH} is set to \tcode{128} when compiling
with \tt{--target=wasm32-unknown-unknown}.
}

WG21 can define the \tcode{BITINT_MAXWIDTH} as whatever they want to;
it is of no consequence because compiler vendors are not going to make that
width available when there is no platform ABI for \tcode{_BitInt(BITINT_MAXWIDTH)}.
If compiler vendors did that,
there would be a risk of a massive future ABI break in order to comply with the system ABI,
once defined.
Without a single platform ABI, there would also be no portable way for code generated
by different compilers to interoperate,
such as compiling a C library with GCC and using it from Clang-compiled C++ code.

An increase to the \tcode{BITINT_MAXWIDTH} is political posturing.
That does not mean that it's entirely pointless.
If C++ defined the minimum to be, say, \tcode{65'535},
this would motivate platforms to define an ABI for large bit-precise integers.

\h4{Possible increased \tt{BITINT_MAXWIDTH} values}

Firstly, it should be noted that \ref(P3140R0) got substantial criticism
just for attempting to standardize 128-bit integers for embedded developers.
As a compromise, it may be reasonable to increase the \tcode{BITINT_MAXWIDTH}
only for hosted implementations, not for freestanding implementations.
That being said, there two plausible increased minimums:

\ul{
  \li{
    \tcode{128}. Many platform ABIs (see example above)
    already define an ABI for \tcode{_BitInt(128)}.
    128-bit integers have been provided by compilers for a long time now,
    at least by GCC and Clang (\tcode{__int128}).
    There are heaps of motivation (see \ref(P3140R0)) for 128-bit computation.
    The calling conventions are also relatively obvious for 64-bit platforms:
    pass via pair of 64-bit integers.
  }
  \li{
    \tcode{65'535}.
    Both GCC and Clang already support this width.
    Some cryptographic use cases like future-proof RSA computations
    need 8192 bits of key size,
    and at least double that for modular arithmetic.
    It is unlikely that a cryptographic library needs 4096 bits but does not need 8192 bits
    at any point,
    but likely that 65,535 is sufficiently large, even in the next few years.
  }
}

Beyond that, \tcode{_BitInt} may be tricky to use.
When working with Clang's \tcode{_BitInt(8'388'608)},
a single \tcode{+} operation could result in stack overflow because the result is 1 MiB large.
The user would have to carefully ensure that all objects (including temporaries)
have static or dynamic storage duration (i.e. use \tcode{new} or global variables).
For these extreme sizes, a dynamically sized integer is more ergonomic.
Therefore, setting the minimum to millions feels unmotivated.

\h3{Template argument deduction}

The following code should be valid:

\cppblock{
template <std::size_t N>
  void f(std::bit_int<N>);

int main() {
  f(std::bit_int<3>{}); // OK, N = 3
}
}

This would be a consequence of deduction from \tcode{_BitInt} being valid:

\cppblock{
template <unsigned N>
  void f(_BitInt(N));
template <int N>
  void g(_BitInt(N));

int main() {
  f(_BitInt(3)(0)); // OK, N = 3
  g(_BitInt(3)(0)); // OK, N = 3
}
}

This behavior is already implemented by Clang as a C++ compiler extension,
and makes deduction behave identically to deducing sizes of arrays.
In general, the aim is to make the deduction of \tcode{_BitInt} widths
as similar as possible to arrays because users are already familiar with the latter.
It is also clearly useful because it allows writing templates
that can accept \tcode{_BitInt} of any width.

While this behavior could arguably be excluded from the MVP,
it would be extremely surprising to users if such deduction was not possible,
given that appearance of \tcode{std::bit_int}.
If deducing \tcode{N} from \tcode{std::array<T, N>} is possible,
why would it not be possible to deduce \tcode{N} from \tcode{std::bit_int<N>}?

One thing deliberately not allowed is:
\cppblock{
_BitInt x = 123wb;
std::bit_int y = 123wb;
}

This class-template-argument-deduction-like construct is not part of the MVP and if desired,
should be proposed separately.
Even if it was allowed, \tcode{std::bit_int} is proposed to be an alias template,
and alias templates do not support "forwarding deduction" to CTAD.

\h3(id=preprocessor){No preprocessor changes, for better or worse}

To my understanding, no changes to the preprocessor are required.
\ref(N2763) did not make any changes to the C preprocessor either.
In most contexts, integer literals in the preprocessor are simply a \grammarterm{pp-number},
and their numeric value or type is irrelevant.

Within the controlling constant expression of an \tcode{#if} directive,
all signed and unsigned integer types
behave like \tcode{intmax_t} and \tcode{uintmax_t}\iref{cpp.cond},
which may be surprising.

\Bex{
The following code is ill-formed
if \tcode{intmax_t} is a 64-bit signed integer (which it is on many platforms):
\cppblock{
#if \cowel_highlight(cpp){1'000'000'000'000'000'000'000'000wb} // error
#endif
_BitInt(81) x = 1'000'000'000'000'000'000'000'000wb; // OK
}
\tcode{#if \cowel_highlight(cpp){1'000'000'000'000'000'000'000'000wb}} is ill-formed
because the integer literal is of type \tcode{_BitInt(81)},
which behaves like \tcode{intmax_t} within \tcode{#if}.
Since \math{\msup{\mn{10}\mn{32}}} does not fit within \tcode{intmax_t},
the literal is ill-formed\iref{lex.icon#4}.
}

The current behavior could be seen as suboptimal
because it makes bit-precise integers dysfunctional within the preprocessor.
However, the preprocessor is largely "owned" by C,
and any fix should go through WG14.
In any case, fixing the C preprocessor is not part of the MVP.

\:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

\h2{Library design}

When discussing library design,
it is important to understand that the vast majority of support for bit-precise integers
"sneaks" into the standard without any explicit design changes or wording changes.
Many existing facilities (e.g. \header{bit}) support any integer type;
adding bit-precise integers to the core language silently adds library support.
The following sections deal mostly with areas of the standard
where some explicit design changes must be made.

\Bnote{
See \ref(#impact-on-the-standard-library)
for a complete list of changes,
including such silently added support.
}

\h3{Naming of the alias template}

The approach is to expose bit-precise integers via two alias templates:

\cppblock{
template <size_t N>
  using bit_int = _BitInt(N);
template <size_t N>
  using bit_uint = unsigned _BitInt(N);
}

The goal is to have a spelling reminiscent of the C \tcode{_BitInt} spelling.
There are no clear problems with it,
so it is the obvious candidate.
\tt{int} and \tt{uint} match the naming scheme of existing aliases,
such as \tcode{intN_t}, \tcode{uint_fastN_t}, etc.

The alias names as a whole also act as abbreviations of the core language term
(which is copied from C):
\ul{
  \li{\tcode{bit_int<N>} is a bit-precise signed integer of width \tcode{N}}
  \li{\tcode{bit_uint<N>} is a bit-precise unsigned integer of width \tcode{N}}
}

\h4{Why no \tcode{_t} suffix?}

While the \tcode{_t} suffix would be conventional for simple type aliases
such as \tcode{uint32_t},
there is no clear precedent for alias templates.
There are alias templates such as \tcode{expected::rebind}
without any \tcode{_t} or \tcode{_type} suffix,
but "type trait wrappers" such as \tcode{conditional_t} which have a \tcode{_t} suffix.

The \tcode{_t} suffix does not add any clear benefit,
adds verbosity,
and distances the name from the C spelling \tcode{_BitInt}.
Brevity is important here because \tcode{bit_int}
is expected to be a commonly spelled type.
A function doing some bit manipulation could use this name numerous times.

\h3(id=printing-support){\tt{format}, \tt{to_chars}, and \tt{to_string} support for bit-precise integers}

I consider printing support to be part of the MVP for bit-precise integers.
There are numerous reasons for this:

\ul{
  \li{
    Being able to print bit-precise integers is clearly useful.
    It seems unthinkable that it would not be supported at some point,
    even if support was not added by this proposal.
  }
  \li{
    It would take considerable wording effort to exclude
    support for bit-precise integers from these facilities,
    only for it to be reverted once support is inevitably added.
    For example, the expression \tcode{std::println("{:b}", 100)} prints \tt{1100100}.
    This is specified in terms of \tcode{std::to_chars} where \tcode{base = 2}.
    If \tcode{std::to_chars} does not actually support bit-precise integers,
    this wording becomes nonsensical.
  }
  \li{
    The design is obvious.
    No changes to \tcode{format} are necessary
    if \tcode{to_chars} supports bit-precise integers.
  }
}

To facilitate printing and parsing,
the following function templates are added:

\cppblock{
template<class T>
  constexpr to_chars_result to_chars(char* first, char* last, T value, int base = 10);
template<class T>
  constexpr from_chars_result from_chars(char* first, char* last, T& value, int base = 10);

template<class T>
  string to_string(T val);
template<class T>
  wstring to_wstring(T val);
}

\Bnote{
The \tcode{to_string} and \tcode{to_wstring} overloads for integral types
are made \tcode{constexpr} by \ref(P3438R0).
If that paper be accepted,
the overloads for bit-precise integers should also be made \tcode{constexpr} for consistency.
}

\Bnote{
See also \ref(#passing-bitint) for an explanation of why this function passes by value.
}

\tcode{T} is constrained to accept any bit-precise integer type.
It would have also been possible to accept two overloads
taking \tcode{bit_int<N>} and \tcode{bit_uint<N>} with some constant template argument instead,
but this doubles the amount of declarations without any clear benefit.

Such a signature is also more future-proof:
the constraints can be relaxed if more types are supported (e.g. extended integer types),
whereas a \tcode{bit_int<N>} parameter can only support bit-precise integer,
until the end of times.
For parsing and printing, this seems short-sighted.

It should also be noted that the existing overloads such as \tcode{to_string(int)}
cannot be removed because it would break existing code.

\Bex{
Wrapper types which are convertible to \tcode{int} (but are not \tcode{int})
may rely on these dedicated overloads:
\cppblock{
struct int_wrapper {
  int x;
  operator int() const { return x; }
};

string to_string(int);
string to_string_generic(integral auto);

to_string(int_wrapper{});         // OK
to_string_generic(int_wrapper{}); // error: integral<int_wrapper> constraint not satisfied
}

Analogously, if we replaced all the non-template overloads
and handled all integers in a single function template,
this may break existing valid calls to \tcode{to_string} etc.
}

\h3(id=preventing-iota-view-abi-break){Preventing \tt{ranges::iota_view} ABI break}

Due to the current wording in \eelis{range.iota.view#1},
adding bit-precise integers or extended integers of greater width than \tcode{long long}
potentially forces the implementation to redefine
\tcode{ranges::iota_view::iterator::difference_type}.
Changing the type would be an ABI break.
This problem is similar to historical issues with \tcode{intmax_t},
where adding 128-bit integers would force the implementation to redefine the former type.

To prevent this, the proposal tweaks the wording in \ref(#range.iota.view)
so that new extended or bit-precise integers may be added.
Dealing with extended integer types extends slightly beyond the scope of the MVP,
but it would be silly to leave the wording in an undesirable state,
where adding a 128-bit extended integer still forces an ABI break.

\h3{Bit-precise \tcode{size_t}, \tcode{ptrdiff_t}}

As in C,
the proposal allows for \tcode{size_t} and \tcode{ptrdiff_t} to be bit-precise integers,
which is a consequence of \tcode{sizeof} and pointer subtraction
potentially yielding a bit-precise integer.

Whether bit-precise integers in those places is desirable is for implementers and users to decide,
but from the perspective of the C standard and the C++ standard,
there is no compelling reason to disallow it.
It would be a massive breaking change if existing C++ implementations redefined
the type of these,
so it is unlikely we will see an implementation that makes use of this freedom anytime soon.

\h3(id=bitint-abs){New \tt{abs} overload}

The proposal adds the following \tcode{abs} overload:

\cppblock{
template<size_t N>
  constexpr bit_int<N> abs(bit_int<N> j);
}

While \tcode{abs} is not strictly part of the MVP,
taking the absolute of an integer is such a fundamental, easy-to-implement, and useful
operation that we may as well include it here.

\Bnote{
See \ref(#passing-bitint) for an explanation as to why this signature is chosen.
}

\h3(id=bitint-cmath){Using bit-precise integers in \tt{<cmath>} functions}

The proposal adds support for using bit-precise integers in all \tt{<cmath>} functions:

\cppblock{
std::sqrt(0);    // OK, int → call to std::sqrt(double)
std::sqrt(0wb);  // OK, _BitInt(1) → call to std::sqrt(double)
}

This is done simply for consistency with C:
after some consulting with WG14 members,
I am under the impression that C's \header{tgmath.h} functions deliberately
all integers types (including bit-precise integers),
not just as the result of defective wording.
Consequently, \tcode{_BitInt} can be passed both to
the type-generic \tcode{\hl(macro){sqrt}} macro
as well as to the regular \tcode{sqrt(double)} function.

\h3(id=random-support){Lack of random number generation support}

Support for random number generation is not added
because too many design changes are required,
with non-obvious decisions.
Users can also live without bit-precise integer support in \header{random}
for a while, so this feature is not part of the MVP.

Wording changes would be needed because \header{random}
specifically supports certain integer types specified in \eelis{rand.req.genl},
rather than having blanket support for bit-precise integers.

Another issue lies with \tcode{linear_congruential_engine}.
This generator performs modular arithmetic,
which requires double the integer width for intermediate results.
For example, \tcode{int64_t} modular arithmetic is implemented using \tcode{__int128}
in some standard libraries (if available).
An obvious problem for bit-precise integers is how modular arithmetic
for \tcode{bit_int<BITINT_MAXWIDTH>} is meant to be implemented.
We obviously can't just use a wider integer type because none exists.
These and other potential design issues should be explored in a separate paper.

\h3(id=simd-support){\tt{simd} support for bit-precise integers}

\header{simd} is one of the few parts in the standard library
where the implementation is highly specific to integer widths,
at least if high implementation quality is needed.

\h4{\tt{simd} design problems}

There are many important questions, such as:

\ul{
  \li{
    How do we best optimize \tcode{simd::vec<bit_int<1>>}?
    that type is effectively a bitset.
  }
  \li{
    How do we deal with padding bits?
    Standard integers are typically padding-free, but bit-precise integers are not
    except for specific sizes.
  }
  \li{
    What about \tcode{simd::vec<bit_int<512>>} or even greater widths?
    It seems like this case degenerates into a scalar implementation anyway.
  }
}

It is not obvious whether design changes are needed to properly
support bit-precise integers.
Furthermore, adding a naive implementation for e.g. \tcode{bit_int<1>}
would result in an ABI break when being replaced with a more efficient "bit-packed"
implementation later.

\h4{\tt{simd} design conclusion}

Due to these design concerns,
I do not consider \em{full}\tcode{simd} support to be part of the MVP.
However, \tcode{simd} support for bit-precise integers is clearly useful,
so a compromise is possible:
\tcode{simd} support is added only for those bit-precise integers
whose width matches a standard integer type.

This means that a \tcode{simd::vec<bit_int<32>>} implementation
"piggy-backs" off of an existing \tcode{simd::vec<int>} implementation,
assuming \tcode{int} is a 32-bit signed integer.
Such limited support is easy to provide.

\Bnote{
This restriction is inspired by the constraints (inherited from C) on
\tcode{stdc_count_ones} and other \header{stdbit.h} functions.
Those functions accept standard integers
as well as bit-precise integers of matching width.
}

\h3(id=valarray-support){\tt{valarray} support for bit-precise integers}

Bit-precise integer support in \tcode{valarray} is required.
While the same concerns as with \header{simd} apply \em{in theory},
it is easy to provide a naive implementation,
and the implementation in standard libraries is typically naive anyway,
including for existing integers.

\Bnote{
Naive means that in libc++, libstdc++, and the MSVC STL,
operator overloads such as \tcode{valarray::operator+}
are implemented as a simple loop
rather than being manually optimized with SIMD operations.
}

\h3(id=broadening-type-traits){Broadening \tt{is_integral}}

Since bit-precise integer types are integral types,
obviously, \tcode{is_integral_v<T>} should be \tcode{true} for any bit-precise integer \tcode{T}.

There is a potential concern that existing C++ code constrained using
\tcode{is_integral} or \tcode{integral} never anticipated that the templates
would be instantiated with huge integers like \tcode{bit_int<1024>}.
That is simply a problem we have to live with.
The only way to avoid the issue would be to create a taxonomy of integer types
that is confusing and inconsistent with C
(e.g. by not considering bit-precise integers to be integral types),
or to make \tcode{is_integral_v} inconsistent with the term "integral type".
Both of these alternatives seem terrible.

\h3{\tt{make_signed} and \tt{make_unsigned}}

To prevent breaking existing code,
the behavior of \tcode{make_signed} and \tcode{make_unsigned} needs to be
made future-proof:

\cppblock{
make_unsigned_t<char32_t> // previously unsigned int, becomes _BitInt(32) unless we reword
}

The rank of \tcode{unsigned int} is greater than the rank of
\tcode{unsigned _BitInt(32)} (assuming those have the same width; see \eelis{conv.rank}).
Therefore, \tcode{make_unsigned_t} would need to be \tcode{unsigned _BitInt(32)},
since it produces\iref{meta.trans.sign}

\Bquote{
unsigned integer type with smallest rank\iref{conv.rank}
for which \tcode{sizeof(T) == sizeof(type)}
}

Furthermore, the current wording would
give the user an implementation-defined type in the following scenario:

\cppblock{
enum E : _BitInt(32) { };
make_signed_t<E> x; // might be _BitInt(32)
}

\tcode{make_signed_t<E>} could be either \tcode{_BitInt(32)} or an extended integer type
with lower conversion rank than \tcode{_BitInt(32)}.
However, for simplicity, \tcode{make_signed} and \tcode{make_unsigned}
should always produce a bit-precise integer type when they are fed a bit-precise integer type
or an enumeration whose underlying type is a bit-precise integer.

Overall, \tcode{make_signed} can be made future-proof with the following set of rules:

\ul{
  \li{
    For signed integers and unsigned integers,
    it does the "obvious thing".
  }
  \li{
    For types whose underlying type is a bit-precise integer,
    it behaves like \tcode{make_signed_t<underlying_type_t<T>>}.
    This only affects enumeration types, since integral types like \tcode{char32_t}
    are currently specified not to have a bit-precise underlying type.
  }
  \li{
    For any other integral type (\tcode{char32_t}, other enumerations, etc.),
    it denotes the smallest \b{standard or extended} signed integer type that fits
    that integral type.
  }
}

\tcode{make_unsigned} should behave correspondingly.

\Bnote{
See \ref(#meta.trans.sign) for wording.
}

\h3(id=misc-support){Miscellaneous library support}

There are many more standard library parts to which support for bit-precise integers is added.
Examples include:

\ul{
  \li{
    C headers such as \header{stdbit.h} and \header{stdckdint.h}
    receive the same degree of support as they have in C.
    This is the obvious design, and any deviation would need to be justified somehow.
  }
  \li{
    Various utilities such as \tcode{to_integer} and \tcode{hash}
    receive support for bit-precise integers.
    It is implausible that support wouldn't be added in the long run,
    and the support is added by extending the existing blanket support for integers;
    no wording changes are needed.
  }
}

\h3{Feature testing}

After consulting with some LWG and SG10 experts,
I have opted to add only two feature-test macros:
one for the core feature,
and one for the standard library.
While more granular feature-testing could be useful considering
that the feature is quite large,
there seems to be little enthusiasm for it.

\h3(id=passing-bitint){Passing \tt{bit_int} into standard library function templates}

Unlike standard integers,
it is plausible that some bit-precise integers are too large to be passed on the stack,
or at least too large to make this the "default option".
Nonetheless,
all proposed library functions which operate on \tcode{bit_int}
should accept \tcode{bit_int} by value.

\Bex{
The proposal adds this \tcode{abs} overload:
\cppblock{
template<size_t N>
  constexpr bit_int<N> abs(bit_int<N> j);
}

If implemented verbatim like this,
in the \ref(https://gitlab.com/x86-psABIs/x86-64-ABI){x86-64 psABI},
\tcode{bit_int<64>} would be passed via single register,
\tcode{bit_int<128>} would be passed via a pair of registers,
and any wider integer integer would be pushed onto the stack.
Passing via stack is questionable and may result in an immediate program crash
when millions of bits are involved.
}

The reason for having such signatures
is that the details of how values are passed into functions
are outside the scope of the standard.
Since most functions in the standard are not addressable,
and since we don't care about keeping the results of reflecting on the standard library stable,
the actual overload sets in the library implementation can differ from the declarations
in the standard.

\Bex{
An implementation of the \tcode{abs} function template could look as follows:

\cppblock{
template<size_t __n>
constexpr _BitInt(n) abs(_BitInt(__n) __j) { // pass small integers by value
  return __j >= 0 ? __j : -__j;
}

template<size_t __n>
  requires (sizeof(_BitInt(__n)) > __pass_by_value_max_size)
constexpr _BitInt(n) abs(const _BitInt(__n)& __j) { // pass large integers by reference
  return __j >= 0 ? __j : -__j;
}
}

Another plausible implementation strategy is to
use an ABI-altering, implementation-specific attribute.
\cppblock{
template<size_t __n>
constexpr _BitInt(n) abs([[impl::pass_large_by_ref]] _BitInt(__n) __j) {
  return __j >= 0 ? __j : -__j;
}
}
Such an attribute could alter the ABI for \tcode{__j}
so that it is passed indirectly (via address) beyond a certain size,
not on the stack.
}

Admittedly, having the standard pass all integers by value may give the user
the false impression that a \tcode{bit_int<N>} function parameter (with unconstrained \tcode{N})
is idiomatic and harmless, which is problematic.
However, it is seemingly the lesser evil,
since the alternative is wasting LEWG and LWG time on quality of implementation.

Even if \tcode{bit_int} was passed into standard library functions by reference,
the same issue arises for return types:
\tcode{bit_int} would be returned by value from \tcode{std::abs}, \tcode{std::rotl},
\tcode{std::gcd}, \tcode{std::simd::vec::operator[]},
library types that use a bit-precise \tcode{size_type} or \tcode{difference_type},
and many more.

\:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

\h2{Education}

Following SG20 Education recommendations at Sofia 2025,
this proposal contains guidance on how bit-precise integers
are meant to be taught by learning resources.

\h3{Teaching principles}

\ol{
  \li{
    Emphasize familiar features.
    The closest equivalents to \tcode{std::bit_int} and \tcode{std::bit_uint}
    are \tcode{std::intN_t} and \tcode{std::uintN_t}, respectively.
  }
  \li{
    Clearly distinguish \tcode{std::bit_int} from other existing integer types.
    It should be clarified that \tcode{std::bit_int} is always a distinct type
    from the \tcode{std::intN_t} aliases, even if it behaves similarly.
    Furthermore, the major differences are:
    \ul{
      \li{
        \tcode{std::bit_int} is not optional (though there exists a maximum width),
        whereas any \tcode{std::intN_t} may not actually exist.
      }
      \li{
        \tcode{std::bit_int} is not subject to integer promotion,
        unlike any of the existing standard integer types.
      }
      \li{
        \tcode{std::bit_int} cannot be used as the underlying type of enumerations.
      }
    }
  }
  \li{
    Only reference the \tcode{_BitInt} spelling in a note on C compatibility.
    \tcode{_BitInt(N)} looks nothing like the class templates that C++ users are used to,
    and nothing suggests that \tcode{N} is required to be a constant expression.
    The \tcode{std::bit_int} and \tcode{std::bit_uint} alias templates
    should be taught first and foremost.
  }
  \li{
    Point out potential pitfalls:
    \ul{
      \li{
        \tcode{std::bit_int} has a \tcode{BITINT_MAXWIDTH} which is not guaranteed
        to be any more than \tcode{64}.
        The user should be made aware of this portability problem.
      }
      \li{
        When writing generic code,
        the user should be made aware that accepting \tcode{std::bit_int<N>}
        in a function signature may be problematic.
        For all they know, \tcode{std::bit_int<N>} could have millions of bits,
        and this could make the type too large for passing on the stack.
      }
    }
  }
}

\:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

\h2{Implementation experience}

\tcode{_BitInt}, formerly known as \tcode{_ExtInt}, has been a compiler extension
in Clang for several years now.
The core language changes are essentially standardizing that compiler extension.

\:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

\h2{Impact on the standard}

\h3{Impact on the core language}

The core language changes essentially boil down to adding the \tcode{_BitInt}
type and the \tcode{wb} \grammarterm{integer-suffix}.
This obviously comes with various syntax changes,
definitions of conversion rank,
addition of template argument deduction rules, etc.
The vast majority of core language wording which deals with integers
is not affected by the existence of bit-precise integers.

\h3{Impact on the standard library}

The impact of adding bit-precise integers to the standard library is quite enormous
because there are many parts of the library
which already support any integer type via blanket wording.
Additionally, bit-precise integer support for various components such as \tcode{std::to_chars}
is explicitly added.

Since this proposal does not explicitly remove support for bit-precise integers,
support "sneaks" its way in, without any explicit wording changes.
For example, use of bit-precise integers in \header{simd}, \header{bit}, \header{valarray},
and many others is enabled.

The addition of bit-precise integers means that (as in the core language),
the \tcode{size_type} of various containers may be a bit-precise integer,
\tcode{size_t} and \tcode{ptrdiff_t} may be bit-precise integers, etc.

Find a summary of affected library components below.
In the interest of reducing noise,
the possible changes to container \tcode{size_type}s are not listed.

\table(class=wording){
  \tr{
    \th{Header}
    \th{Changes}
    \th{Wording}
    \th{See also}
  }
  \tr{
    \td{\header{algorithm}}
    \td{Relax some \i{Mandates} due to implementability problems.}
    \td{\ref(#alg.foreach)}
    \td{\ref(#misc-support)}
  }
  \tr{
    \td{\header{atomic}}
    \td{
      Add support for bit-precise integers.
      Partial specializations for integral types also support \tcode{bit_int}.
    }
    \td{\ref(#atomics.ref.int),\br\ref(#atomics.types.int)}
    \td{\ref(#misc-support)}
  }
  \tr{
    \td{\header{bit}}
    \td{Expand blanket support for integers.}
    \td{None required}
    \td{\ref(#misc-support)}
  }
  \tr{
    \td{\header{charconv}}
    \td{Add \tcode{to_chars} and \tcode{from_chars} overloads.}
    \td{\ref(#charconv.syn)}
    \td{\ref(#printing-support)}
  }
  \tr{
    \td{\header{chrono}}
    \td{Bit-precise integers can be used in e.g. \tcode{duration}.}
    \td{None required}
    \td{}
  }
  \tr{
    \td{\header{climits}}
    \td{Add \tcode{BITINT_MAXWIDTH} macro.}
    \td{\ref(#climits.syn)}
    \td{}
  }
  \tr{
    \td{\header{cmath}}
    \td{
      Add \tcode{abs} overload.
      Allow passing bit-precise integers to most math functions.
    }
    \td{\ref(#cmath.syn)}
    \td{\ref(#bitint-abs),\br\ref(#bitint-cmath)}
  }
  \tr{
    \td{\header{complex}}
    \td{Expand blanket support for integers.}
    \td{None required}
    \td{\ref(#misc-support)}
  }
  \tr{
    \td{\header{concepts}}
    \td{Some concepts broadened (e.g. \tcode{integral}).}
    \td{None required}
    \td{\ref(#broadening-type-traits)}
  }
  \tr{
    \td{\header{format}}
    \td{Expand blanket support for integers.}
    \td{None required}
    \td{\ref(#printing-support)}
  }
  \tr{
    \td{\header{limits}}
    \td{\tcode{numeric_limits} specializations required as blanket support.}
    \td{None required}
    \td{}
  }
  \tr{
    \td{\header{limits.h}}
    \td{Changed indirectly.}
    \td{\ref(#climits.syn)}
    \td{\header{climits}}
  }
  \tr{
    \td{\header{linalg}}
    \td{Expand blanket support for integers.}
    \td{None required}
    \td{}
  }
  \tr{
    \td{\header{mdspan}}
    \td{Bit-precise integers may be used as an index type.}
    \td{None required}
    \td{}
  }
  \tr{
    \td{\header{meta}}
    \td{Some queries broadened (e.g. \tcode{is_integral_type}).}
    \td{None required}
    \td{\ref(#broadening-type-traits)}
  }
  \tr{
    \td{\header{numeric}}
    \td{Expand blanket support for integers (\tcode{gcd}, saturating arithmetic, etc.)}
    \td{None required}
    \td{}
  }
  \tr{
    \td{\header{ranges}}
    \td{
      Change \tcode{\exposid{IOTA-DIFF-T}} to prevent ABI break
      when integer types are added.
    }
    \td{\ref(#range.iota.view)}
    \td{\ref(#preventing-iota-view-abi-break)}
  }
  \tr{
    \td{\header{simd}}
    \td{Add limited support for bit-precise integers.}
    \td{\ref(#simd.general)}
    \td{\ref(#simd-support)}
  }
  \tr{
    \td{\header{stdbit.h}}
    \td{Inherit bit-precise integer support from C.}
    \td{\ref(#stdbit.h.syn)}
    \td{\ref(#misc-support)}
  }
  \tr{
    \td{\header{stdckdint.h}}
    \td{Inherit bit-precise integer support from C.}
    \td{\ref(#numerics.c.ckdint)}
    \td{\ref(#misc-support)}
  }
  \tr{
    \td{\header{string}}
    \td{Add \tcode{to_string} and \tcode{to_wstring} overloads.}
    \td{\ref(#string.conversions)}
    \td{\ref(#printing-support)}
  }
  \tr{
    \td{\header{tgmath.h}}
    \td{Changed indirectly.}
    \td{None required}
    \td{\header{cmath}, \header{complex}}
  }
  \tr{
    \td{\header{type_traits}}
    \td{Some traits broadened (e.g. \tcode{is_integral}).}
    \td{None required}
    \td{\ref(#broadening-type-traits)}
  }
  \tr{
    \td{\header{utility}}
    \td{Expand blanket support for integers (e.g. \tcode{to_integer}).}
    \td{None required}
    \td{\ref(#misc-support)}
  }
  \tr{
    \td{\header{valarray}}
    \td{Expand blanket support for integers.}
    \td{None required}
    \td{\ref(#valarray-support)}
  }
  \tr{
    \td{\header{version}}
    \td{Add feature-test macros.}
    \td{\ref(#version.syn)}
    \td{}
  }
}

\Bnote{
There are numerous other standard library facilities which now support bit-precise integers,
but are not mentioned specially because they are not numeric in nature.
For example, it is possible to store a \tcode{bit_int} in \tcode{any},
but \header{any} is not mentioned specially in the table above.
}

\Bnote{
See \eelis{headers} and \eelis{support.c.headers.general}
for a complete list of headers.
}

\:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

\h2{Wording}

The following changes are relative to \ref(N5014).

\:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

\h3{Core}

\style{
  math[display=inline] {
    font-size: inherit;
  }

  .wording td {
    vertical-align: top;
    border-bottom: 1px solid var(--border-color);
  }

  table.wording code-block {
    margin: 0;
  }
}

\cowel_macro(ins_quotes){\cowel_to_html{\ins{\N{LEFT DOUBLE QUOTATION MARK}}}\cowel_put\cowel_to_html{\ins{\N{RIGHT DOUBLE QUOTATION MARK}}}}
\cowel_macro(bit_int_of_width){\N{LEFT DOUBLE QUOTATION MARK}\tcode{_BitInt} of width \cowel_put\N{RIGHT DOUBLE QUOTATION MARK}}
\cowel_macro(bit_uint_of_width){\N{LEFT DOUBLE QUOTATION MARK}\tcode{unsigned _BitInt} of width \cowel_put\N{RIGHT DOUBLE QUOTATION MARK}}

\Bdecision{
CWG needs to decide what the \q{quoted} (prose)
spelling of bit-precise integer types should be.
The current spelling is e.g. \bit_uint_of_width{\math{\mi{N}}},
which is fairly similar to other code-heavy spellings like \q{\tcode{unsigned int}}.

However, this is questionable because \tcode{_BitInt} is not valid C++ in itself;
\tcode{_BitInt(N)} is.
An alternative would be a pure prose spelling, like
\q{bit-precise unsigned integer of width \math{\mi{N}}},
which is a bit more verbose.

There is no strong author preference.
}

\h4(show-number=false){[lex.icon]}

In \eelis{lex.icon},
change the grammar as follows:

\Bdiff{
\dl(class=grammar){
  \dt{\grammarterm{integer-suffix}:}
  \dd{unsigned-suffix \opt{long-suffix}}
  \dd{unsigned-suffix \opt{long-long-suffix}}
  \dd{unsigned-suffix \opt{size-suffix}}
  \dd{\ins{unsigned-suffix \opt{bit-precise-int-suffix}}}
  \dd{long-suffix \opt{unsigned-suffix}}
  \dd{long-long-suffix \opt{unsigned-suffix}}
  \dd{size-suffix \opt{unsigned-suffix}}
  \dd{\ins{bit-precise-int-suffix \opt{unsigned-suffix}}}
}
\dl(class=grammar){
  \dt{\grammarterm{unsigned-suffix}: one of}
  \dd{\tcode{u} \tcode{U}}
}
\dl(class=grammar){
  \dt{\grammarterm{long-suffix}: one of}
  \dd{\tcode{l} \tcode{L}}
}
\dl(class=grammar){
  \dt{\grammarterm{long-long-suffix}: one of}
  \dd{\tcode{ll} \tcode{LL}}
}
\dl(class=grammar){
  \dt{\grammarterm{size-suffix}: one of}
  \dd{\tcode{z} \tcode{Z}}
}
\dl(class=grammar){
  \dt{\ins{\grammarterm{bit-precise-int-suffix}: one of}}
  \dd{\ins{\tcode{wb} \tcode{WB}}}
}
}

\editnote{
The name \grammarterm{bit-precise-int-suffix} is identical to the one used in C.
See \ref(N3550) §6.4.5.2 Integer literals.
}

Change table \eelis{tab:lex.icon.type} as follows:

\Bdiff{
\table(class=wording){

  \tr{
    \th{\grammarterm{\nobr{integer-suffix}}}
    \th{\grammarterm{decimal-literal}}
    \th{\grammarterm{integer-literal} other than \grammarterm{decimal-literal}}
  }

  \tr{
    \td{none}
    \td{
      \itemdecl{
\ins_quotes{int}
\ins_quotes{long int}
\ins_quotes{long long int}
}
    }
    \td{
      \itemdecl{
\ins_quotes{int}
\ins_quotes{unsigned int}
\ins_quotes{long int}
\ins_quotes{unsigned long int}
\ins_quotes{long long int}
\ins_quotes{unsigned long long int}
}
    }
  }

  \tr{
    \td{\tcode{u} or \tcode{U}}
    \td{
      \itemdecl{
\ins_quotes{unsigned int}
\ins_quotes{unsigned long int}
\ins_quotes{unsigned long long int}
}
    }
    \td{
      \itemdecl{
\ins_quotes{unsigned int}
\ins_quotes{unsigned long int}
\ins_quotes{unsigned long long int}
}
    }
  }

  \tr{
    \td{\tcode{l} or \tcode{L}}
    \td{
      \itemdecl{
\ins_quotes{long int}
\ins_quotes{long long int}
}
    }
    \td{
      \itemdecl{
\ins_quotes{long int}
\ins_quotes{unsigned long int}
\ins_quotes{long long int}
\ins_quotes{unsigned long long int}
}
    }
  }

  \tr{
    \td{Both \tcode{u} or \tcode{U} and \tcode{l} or \tcode{L}}
    \td{
      \itemdecl{
\ins_quotes{unsigned long int}
\ins_quotes{unsigned long long int}
}
    }
    \td{
      \itemdecl{
\ins_quotes{unsigned long int}
\ins_quotes{unsigned long long int}
}
    }
  }

  \tr{
    \td{Both \tcode{u} or \tcode{U} and \tcode{ll} or \tcode{LL}}
    \td{
      \itemdecl{
\ins_quotes{unsigned long long int}
}
    }
    \td{
      \itemdecl{
\ins_quotes{unsigned long long int}
}
    }
  }

  \tr{
    \td{\tcode{z} or \tcode{Z}}
    \td{
      the signed integer type corresponding to
      \ins{the type named by}
      \tcode{std::size_t}\iref{support.types.layout}
    }
    \td{
      the signed integer type corresponding to
      \ins{the type named by}
      \tcode{std::size_t}\br\br
      \ins{the type named by}
      \tcode{std::size_t}
    }
  }

  \tr{
    \td{Both \tcode{u} or \tcode{U} and \tcode{z} or \tcode{Z}}
    \td{
      \ins{the type named by}
      \tcode{std::size_t}
    }
    \td{
      \ins{the type named by}
      \tcode{std::size_t}
    }
  }
  \tr{
    \td{\ins{\tcode{wb} or \tcode{WB}}}
    \td{
      \ins{\bit_int_of_width{\math{\mi{N}}},
      where \math{\mi{N}} is the lowest integer \math{\mrow{\mo{≥}\mn{1}}}
      so that the value of the literal can be represented by the type}
    }
    \td{
      \ins{\bit_int_of_width{\math{\mi{N}}},
      where \math{\mi{N}} is the lowest integer \math{\mrow{\mo{≥}\mn{1}}}
      so that the value of the literal can be represented by the type}
    }
  }
  \tr{
    \td{
      \ins{Both \tcode{u} or \tcode{U} and\br{}\tcode{wb} or \tcode{WB}}
    }
    \td{
      \ins{\bit_uint_of_width{\math{\mi{N}}},
      where \math{\mi{N}} is the lowest integer \math{\mrow{\mo{≥}\mn{1}}}
      so that the value of the literal can be represented by the type}
    }
    \td{
      \ins{\bit_uint_of_width{\math{\mi{N}}},
      where \math{\mi{N}} is the lowest integer \math{\mrow{\mo{≥}\mn{1}}}
      so that the value of the literal can be represented by the type}
    }
  }
}
}

\editnote{
The existing rows are adjusted for consistency.
We usually aim to use the \q{quoted} spellings of types
like \bit_int_of_width{\math{\mi{N}}}
in core wording
instead of the \grammarterm{type-id} spellings.
Adding a quoted spelling for bit-precise integers would reveal
that the previous rows "incorrectly" use \grammarterm{type-id}s.
}

Change \eelis{lex.icon#4} as follows:

\Bdiff{
Except for \grammarterm{integer-literal}s containing a \grammarterm{size-suffix}
\ins{or \grammarterm{bit-precise-int-suffix}},
if the value of an \grammarterm{integer-literal} cannot be represented
by any type in its list and an extended integer type\iref{basic.fundamental}
can represent its value,
it may have that extended integer type.
\etc

\note{
An \grammarterm{integer-literal} with a \tcode{z} or \tcode{Z} suffix
is ill-formed if it cannot be represented by \tcode{std::size_t}.
\ins{An \grammarterm{integer-literal} with a \tcode{wb} or \tcode{WB} suffix
is ill-formed if it cannot be represented by any bit-precise integer type
because the necessary width is greater than
\tcode{BITINT_MAXWIDTH}\iref{climits.syn}.}
}
}

\h4(show-number=false){[basic.fundamental]}

Change \eelis{basic.fundamental#1} as follows:

\Bdiff{
There are five \dfn{standard signed integer types}:
\q{\tcode{signed char}},
\q{\tcode{short int}},
\q{\tcode{int}},
\q{\tcode{long int}}, and
\q{\tcode{long long int}}.
In this list,
each type provides at least as much storage as those
preceding it in the list.
\ins{There is also a distinct \dfn{bit-precise signed integer type}
\bit_int_of_width{\math{\mi{N}}}
for each \math{\mrow{\mn{1}\mo{≤}\mi{N}\mo{≤}\mtext{\tt{BITINT_MAXWIDTH}}}}\iref{climits.syn}.}
There may also be implementation-defined
\dfn{extended signed integer types}.
The standard\ins{, bit-precise,} and extended signed integer types are collectively called
\dfn{signed integer types}.
The range of representable values for a signed integer type is
\math{
  \msup{
    \mn{-2}
    \mrow{\mi{N}\mo{−}\mn{1}}
  }
}
to
\math{
  \mrow{
    \msup{
      \mn{2}
      \mrow{\mi{N}\mo{−}\mn{1}}
    }
    \mo{−}
    \mn{1}
  }
}
(inclusive),
where \math{\mi{N}} is called the \dfn{width} of the type.

\note{
Plain \tcode{int}s are intended to have
the natural width suggested by the architecture of the execution environment;
the other signed integer types are provided to meet special needs.
}
}

\editnote{
This change deviates from C at the time of writing;
C2y does not yet allow \tcode{_BitInt(1)},
but may allow it following \ref(N3699).
}

Change \eelis{basic.fundamental#2} as follows:

\Bdiff{
For each of the standard signed integer types,
there exists a corresponding (but different)
\dfn{standard unsigned integer type}:
\q{\tcode{unsigned char}},
\q{\tcode{unsigned short}},
\q{\tcode{unsigned int}},
\q{\tcode{unsigned long int}}, and
\q{\tcode{unsigned long long int}}.
\ins{For each bit-precise signed integer type
\bit_int_of_width{\math{\mi{N}}},
there exists a corresponding \dfn{bit-precise unsigned integer type}
\bit_uint_of_width{\math{\mi{N}}}.}
\del{Likewise, for} \ins{For} each of the extended signed integer types,
there exists a corresponding \dfn{extended unsigned integer type}.
The standard\ins{, bit-precise,} and extended unsigned integer types
are collectively called \dfn{unsigned integer types}.
An unsigned integer type has the same width \math{\mi{N}}
as the corresponding signed integer type.
The range of representable values for the unsigned type is
\math{\mn{0}} to
\math{
  \msup{
    \mn{2}
    \mrow{\mi{N}\mo{−}\mn{1}}
  }
} (inclusive);
arithmetic for the unsigned type is performed modulo \math{\msup{\mn{2}\mi{N}}}.

\note{
Unsigned arithmetic does not overflow.
Overflow for signed arithmetic yields undefined behavior\iref{expr.pre}.
}
}

\comment{
Change \eelis{basic.fundamental#4} as follows:

\Bdiff{
The width of each standard signed integer type
shall not be less than the values specified in \tref{basic.fundamental.width}.

The value representation of a signed or unsigned integer type
comprises \math{\mi{N}} bits,
where \math{\mi{N}} is the respective width.
Each set of values for any padding bits\iref{basic.types.general}
in the object representation are
alternative representations of the value specified by the value representation.

\note{
Padding bits have unspecified value, but cannot cause traps.
In contrast, see \IsoC{} 6.2.6.2.
}

\note{
The signed and unsigned integer types satisfy
the constraints given in \IsoC{} 5.2.4.2.1.
}

Except as specified above,
the width of a signed or unsigned integer type is
implementation-defined.
}
}

Change \eelis{basic.fundamental#5} as follows:

\Bdiff{
\etc
The standard signed integer types and standard unsigned integer types
are collectively called the \dfn{standard integer types}\del{, and the}
\ins{. The bit-precise signed integer types and bit-precise unsigned integer types
are collectively called the \dfn{bit-precise integer types}. The}
extended signed integer types and extended
unsigned integer types are collectively called the
\dfn{extended integer types}.
}

\h4(show-number=false){[conv.rank]}

Change \eelis{conv.rank#1} as follows:

\Bdiff{

Every integer type has an \term{integer conversion rank} defined as follows:

\ul{
  \li{
    No two signed integer types other than \tcode{char} and \tcode{signed char}
    (if \keyword{char} is signed) have the same rank, even if they have the same representation.
  }
  \li{
    The rank of a signed integer type is greater than the rank
    of any signed integer type with a smaller width.
  }
  \li{
    The rank of \tcode{long long int} is greater than the rank of \tcode{long int},
    which is greater than the rank of \tcode{int},
    which is greater than the rank of \tcode{short int},
    which is greater than the rank of \tcode{signed char}.
  }
  \li{
    The rank of any unsigned integer type equals the rank of the
    corresponding signed integer type.
  }
  \li{
    The rank of any standard integer type is greater than the rank
    of \ins{any bit-precise integer type with the same width
    and of} any extended integer type with the same width.
  }
  \li{
    The rank of \tcode{char} equals the rank of \tcode{signed char}
    and \tcode{unsigned char}.
  }
  \li{
    The rank of \tcode{bool} is less than the rank of all
    standard integer types.
  }
  \li{
    The ranks of \tcode{char8_t}, \tcode{char16_t}, \tcode{char32_t}, and
    \tcode{wchar_t} equal the ranks of their underlying
    types\iref{basic.fundamental}.
  }
  \li{
    The rank of any extended signed integer type relative to another
    extended signed integer type with the same width
    \ins{and relative to a bit-precise signed integer type with the same width}
    is implementation-defined,
    but still subject to the other rules for determining the integer conversion rank.
  }
  \li{
    For all integer types \tcode{T1}, \tcode{T2}, and \tcode{T3}, if
    \tcode{T1} has greater rank than \tcode{T2} and \tcode{T2} has greater
    rank than \tcode{T3}, then \tcode{T1} has greater rank than
    \tcode{T3}.
  }
}

\note{
The integer conversion rank is used in the definition of the integral
promotions\iref{conv.prom} and the usual arithmetic
conversions\iref{expr.arith.conv}.
}
}

\h4(show-number=false){[conv.prom]}

\editnote{
These changes mirror the C semantics described in
\ref(N3550) §6.3.2.1 Boolean, characters, and integers.
}

Change \eelis{conv.prom#2} as follows:

\Bdiff{
A prvalue that
\ul{
  \li{is not a converted bit-field \del{and} \ins{,}}
  \li{
    has an integer type other than
    \ins{a bit-precise integer type,}
    \tcode{bool}, \tcode{char8_t}, \tcode{char16_t}, \tcode{char32_t},
    or \tcode{wchar_t}\ins{, and}
  }
  \li{
    whose integer conversion rank\iref{conv.rank}
    is less than the rank of \tcode{int}
  }
}
can be converted to
a prvalue of type \tcode{int}
if \tcode{int} can represent all the values of the source type;
otherwise, the source prvalue can be converted to
a prvalue of type \tcode{unsigned int}.
}

Change \eelis{conv.prom#3} as follows:

\Bdiff{
A prvalue of an unscoped enumeration type whose underlying type
is not fixed\ins{\cowel_html_element(sup){1}}
can be converted to a prvalue of the first of the following types
that can represent all the values of the enumeration\iref{dcl.enum}:
\tcode{int},
\tcode{unsigned int},
\tcode{long int},
\tcode{unsigned long int},
\tcode{long long int}, or
\tcode{unsigned long long int}.
If none of the types in that list can represent all the values of the enumeration,
a prvalue of an unscoped enumeration type
\ins{whose underlying type is not a bit-precise integer type}
can be converted
to a prvalue of the extended integer type with lowest integer conversion rank\iref{conv.rank}
greater than the rank of \tcode{long long}
in which all the values of the enumeration can be represented.
If there are two such extended types, the signed one is chosen.

\ins{\cowel_html_element(sup){1)}
This promotion rule excludes bit-precise integers
because the implementation cannot choose
a bit-precise integer type as the underlying type of an enumeration
with no fixed underlying type\iref{dcl.enum}.}
}

Change \eelis{conv.prom#4} as follows:

\Bdiff{
A prvalue of an unscoped enumeration type whose underlying type is fixed\iref{dcl.enum}
can be converted to a prvalue of its underlying type.
Moreover, if integral promotion can be applied to its underlying type,
a prvalue of an unscoped enumeration type whose underlying type is fixed
can also be converted to a prvalue of the promoted underlying type.

\note{
A converted bit-field of enumeration type
is treated as any other value of that type for promotion purposes.
}

\ins{\note{
If the underlying type is a bit-precise integer type,
conversion to a prvalue of that type is possible,
but integral promotion cannot be applied to the underlying type.
}}
}

Change \eelis{conv.prom#5} as follows:

\Bdiff{
A converted bit-field of integral type
\ins{other than a bit-precise integer type}
can be converted to a prvalue of type \tcode{int}
if \tcode{int} can represent all the values of the bit-field;
otherwise, it can be converted to \tcode{unsigned int}
if \tcode{unsigned int} can represent all the values of the bit-field.
}

\h4(show-number=false){[dcl.type.general]}

Change \eelis{dcl.type.general#2} as follows:

\Bdiff{
As a general rule,
at most one \grammarterm{defining-type-specifier} is allowed
in the complete \grammarterm{decl-specifier-seq} of a declaration
or in a \grammarterm{defining-type-specifier-seq},
and at most one \grammarterm{type-specifier} is allowed in a \grammarterm{type-specifier-seq}.
The only exceptions to this rule are the following:
\ul{
  \li{\tcode{const} can be combined with any type specifier except itself.}
  \li{\tcode{volatile} can be combined with any type specifier except itself.}
  \li{
    \tcode{signed} or \tcode{unsigned} can be combined with
    \tcode{char}, \tcode{long}, \tcode{short}, \del{or} \tcode{int}\ins{, or
    a \grammarterm{bit-precise-int-type-specifier}\iref{dcl.type.simple}.}
  }
  \li{\tcode{short} or \tcode{long} can be combined with \tcode{int}.}
  \li{\tcode{long} can be combined with \tcode{double}.}
  \li{\tcode{long} can be combined with \tcode{long}.}
}
}

\h4(show-number=false){[dcl.type.simple]}

Change \eelis{dcl.type.simple#1} as follows:

\Bdiff{
The simple type specifiers are

\dl(class=grammar){
  \dt{\grammarterm{simple-type-specifier}:}
  \dd{\opt{nested-name-specifier} type-name}
  \dd{nested-name-specifier \tcode{template} simple-template-id}
  \dd{computed-type-specifier}
  \dd{placeholder-type-specifier}
  \dd{\ins{bit-precise-int-type-specifier}}
  \dd{\opt{nested-name-specifier} template-name}
  \dd{\tcode{char}}
  \dd{\tcode{char8_t}}
  \dd{\tcode{char16_t}}
  \dd{\tcode{char32_t}}
  \dd{\tcode{wchar_t}}
  \dd{\tcode{bool}}
  \dd{\tcode{short}}
  \dd{\tcode{int}}
  \dd{\tcode{long}}
  \dd{\tcode{signed}}
  \dd{\tcode{unsigned}}
  \dd{\tcode{float}}
  \dd{\tcode{double}}
  \dd{\tcode{void}}

  \dt{\grammarterm{type-name}:}
  \dd{class-name}
  \dd{enum-name}
  \dd{typedef-name}

  \dt{\grammarterm{computed-type-specifier}:}
  \dd{decltype-specifier}
  \dd{pack-index-specifier}
  \dd{splice-type-specifier}

  \dt{\ins{\grammarterm{bit-precise-int-type-specifier}:}}
  \dd{\ins{\tcode{_BitInt} \tcode{(} constant-expression \tcode{)}}}
}
}

\editnote{
The name \grammarterm{bit-precise-int-type-specifier}
is symmetrical with \grammarterm{bit-precise-int-suffix}.
}

Change table \eelis{tab:dcl.type.simple} as follows:

\cowel_macro(dcl_type_simple_row){\tr{\td{\tcode{\cowel_put{0}}}\td{\q{\tcode{\cowel_put{1}}}}}}

\Bdiff{
  \table{
    \tr{
      \th{Specifier(s)}
      \th{Type}
    }
    \tr{
      \td{\grammarterm{type-name}}
      \td{the type named}
    }
    \tr{
      \td{\grammarterm{simple-template-id}}
      \td{the type as defined in \eelis{temp.names}}
    }
    \tr{
      \td{\grammarterm{decltype-specifier}}
      \td{the type as defined in \eelis{dcl.type.decltype}}
    }
    \tr{
      \td{\grammarterm{pack-index-specifier}}
      \td{the type as defined in \eelis{dcl.type.pack.index}}
    }
    \tr{
      \td{\grammarterm{placeholder-type-specifier}}
      \td{the type as defined in \eelis{dcl.spec.auto}}
    }
    \tr{
      \td{\grammarterm{template-name}}
      \td{the type as defined in \eelis{dcl.type.class.deduct}}
    }
    \tr{
      \td{\grammarterm{splice-type-specifier}}
      \td{the type as defined in \eelis{dcl.type.splice}}
    }
    \tr{
      \td{\ins{\tcode{unsigned _BitInt(\math{\mi{N}})}}}
      \td{\ins{\bit_uint_of_width{\math{\mi{N}}}}}
    }
    \tr{
      \td{\ins{\tcode{signed _BitInt(\math{\mi{N}})}}}
      \td{\ins{\bit_int_of_width{\math{\mi{N}}}}}
    }
    \tr{
      \td{\ins{\tcode{_BitInt(\math{\mi{N}})}}}
      \td{\ins{\bit_int_of_width{\math{\mi{N}}}}}
    }
    \dcl_type_simple_row(char, char)
    \dcl_type_simple_row(unsigned char, unsigned char)
    \dcl_type_simple_row(signed char, signed char)
    \dcl_type_simple_row(char8_t, char8_t)
    \dcl_type_simple_row(char16_t, char16_t)
    \dcl_type_simple_row(char32_t, char32_t)
    \dcl_type_simple_row(bool, bool)
    \dcl_type_simple_row(unsigned, unsigned int)
    \dcl_type_simple_row(unsigned int, unsigned int)
    \dcl_type_simple_row(signed, int)
    \dcl_type_simple_row(signed int, int)
    \dcl_type_simple_row(int, int)
    \dcl_type_simple_row(unsigned short int, unsigned short int)
    \dcl_type_simple_row(unsigned short, unsigned short int)
    \dcl_type_simple_row(unsigned long int, unsigned long int)
    \dcl_type_simple_row(unsigned long, unsigned long int)
    \dcl_type_simple_row(unsigned long long int, unsigned long long int)
    \dcl_type_simple_row(unsigned long long, unsigned long long int)
    \dcl_type_simple_row(signed long int, long int)
    \dcl_type_simple_row(signed long, long int)
    \dcl_type_simple_row(signed long long int, long long int)
    \dcl_type_simple_row(signed long long, long long int)
    \dcl_type_simple_row(long long int, long long int)
    \dcl_type_simple_row(long long, long long int)
    \dcl_type_simple_row(long int, long int)
    \dcl_type_simple_row(long, long int)
    \dcl_type_simple_row(signed short int, short int)
    \dcl_type_simple_row(signed short, short int)
    \dcl_type_simple_row(short int, short int)
    \dcl_type_simple_row(short, short int)
    \dcl_type_simple_row(wchar_t, wchar_t)
    \dcl_type_simple_row(float, float)
    \dcl_type_simple_row(double, double)
    \dcl_type_simple_row(long double, long double)
    \dcl_type_simple_row(void, void)
  }
}

Immediately following \eelis{dcl.type.simple#3},
add a new paragraph as follows:

\Bins{
Within a \grammarterm{bit-precise-int-type-specifier},
the \grammarterm{constant-expression} shall be a converted constant expression of type
\tcode{std::size_t}\iref{expr.const}.
Its value \math{\mi{N}} specifies the width
of the bit-precise integer type\iref{basic.fundamental}.
The program is ill-formed unless
\math{
  \mrow{
    \mn{1}
    \mo{≤}
    \mi{N}
    \mo{≤}
    \mtext{\tt{BITINT_MAXWIDTH}}
  }
}\iref{climits.syn}.
}

\editnote{
This added paragraph is inspired by \eelis{dcl.array#1},
which similarly specifies the array size to be a converted constant expression
of type \tcode{std::size_t}.
}

\h4(show-number=false){[dcl.enum]}

\editnote{
The intent is to ban \tcode{_BitInt} from \em{implicitly}
being the underlying type of enumerations,
matching the proposed restrictions in \ref(N3705).
See \ref(#underlying-bitint).
}

Change \eelis{dcl.enum#5} as follows:

\Bdiff{
\etc
If the underlying type is not fixed,
the type of each enumerator prior ot the closing brace is determined as follows:

\ul{
  \li{
    If an initializer is specified for an enumerator,
    the \grammarterm{constant-expression} shall be
    an integral constant expression\iref{expr.const}
    \ins{whose type is not a bit-precise integer type}.
    If the expression has unscoped enumeration type,
    the enumerator has the underlying type of that enumeration type,
    otherwise it has the same type as the expression.
  }
  \li{
    If no initializer is specified for the first enumerator,
    its type is an unspecified signed \del{integral} \ins{integer} type
    \ins{other than a bit-precise integer type}.
  }
  \li{
    Otherwise\ins{,} the type of the enumerator is the same as
    that of the preceding enumerator\ins{,}
    unless the incremented value is not representable in that type,
    in which case the type is an unspecified integral type
    \ins{other than a bit-precise integer type}
    sufficient to contain the incremented value.
    If no such type exists, the program is ill-formed.
  }
}
}

Change \eelis{dcl.enum#7} as follows:

\Bdiff{
For an enumeration whose underlying type is not fixed,
the underlying type is an integral type
that can represent all the enumerator values defined in the enumeration.
If no integral type can represent all the enumerator values,
the enumeration is ill-formed.
It is implementation-defined which integral type is used as the underlying type\ins{,}
except that
\ul{
  \li{
    \ins{the underlying type shall not be a bit-precise integer type and}
  }
  \li{
    the underlying type shall not be larger than \tcode{int}
    unless the value of an enumerator
    cannot fit in an \tcode{int} or \tcode{unsigned int}.
  }
}
If the \grammarterm{enumerator-list} is empty,
the underlying type is as if the enumeration had a single enumerator with value 0.
}

\h4(show-number=false){[temp.deduct.general]}

Add a bullet to \eelis{temp.deduct.general#note-8} as follows:

\Bdiff{
\note{
Type deduction can fail for the following reasons:

\ul{
  \li{Attempting to instantiate a pack expansion containing multiple packs of differing lengths.}
  \li{
    Attempting to create an array with an element type that is \tcode{void},
    a function type, or a reference type,
    or attempting to create an array with a size that is zero or negative.
    \br\example{
    \itemdecl{
template <class T> int f(T[5]);
int I = f<int>(0);
int j = f<void>(0);             // \serif{invalid array}
}
    }
  }
  \li{
    \ins{Attempting to create a bit-precise integer type of invalid width\iref{basic.fundamental}.
    \br\example{
    \itemdecl{
\ins{template <int N> void f(_BitInt(N));
f<0>(0);                        // \serif{invalid bit-precise integer}}
}
    }
    }
  }
  \li{\etc}
}
}
}

\h4(show-number=false){[temp.deduct.type]}

Change \eelis{temp.deduct.type#2} as follows:

\Bdiff{
\etc
The type of a type parameter is only deduced from an array bound
\ins{or bit-precise integer width}
if it is not otherwise deduced.
}

Change \eelis{temp.deduct.type#3} as follows:

\Bdiff{
A given type \tcode{P} can be composed from a number of other types,
templates, and constant template argument values:
\ul{
  \li{
    A function type includes the types of each of the function parameters,
    the return type, and its exception specification.
  }
  \li{
    A pointer-to-member type includes the type of the class object pointed to
    and the type of the member pointed to.
  }
  \li{
    A type that is a specialization of a class template (e.g., \tcode{A<int>}) includes the types,
    templates, and constant template argument values
    referenced by the template argument list of the specialization.
  }
  \li{
    An array type includes the array element type and the value of the array bound.
  }
  \li{
    \ins{A bit-precise integer type includes the integer width.}
  }
}
}

Change \eelis{temp.deduct.type#5} as follows:

\Bdiff{
The non-deduced contexts are:
\ul{
  \li{\etc}
  \li{
    A constant template argument \del{or} \ins{,} an array bound\ins{, or
    a bit-precise integer width,}
    in \ins{any of} which a subexpression references a template parameter.
    \br\ins{\example{
    \itemdecl{
\ins{template<size_t N> void f(_BitInt(N));
template<size_t N> void g(_BitInt(N + 1));
f(100wb);                                   // \serif{OK}, \tcode{N = 8}
g(100wb);                                   // \serif{error: no argument for deduced \tcode{N}}}
}
    }}
  }
  \li{\etc}
}
}

Change \eelis{temp.deduct.type#8} as follows:

\Bdiff{
A type template argument \tcode{T},
a constant template argument \tcode{i},
a template template argument \tcode{TT} denoting a class template or an alias template,
or a template template argument \tcode{VV} denoting a variable template or a concept
can be deduced if \tcode{P} and \tcode{A} have one of the following forms:

\itemdecl{
\opt{\cv} T
T*
T&
T&&
\opt{T}[\opt{i}]
\ins{_BitInt(\opt{i})}
\opt{T}(\opt{T}) noexcept(\opt{i})
\opt{T} \opt{T}::*
\opt{TT}<T>
\opt{TT}<i>
\opt{TT}<TT>
\opt{TT}<VV>
\opt{TT}<>
}

where \etc
}

Do \u{not} change \eelis{temp.deduct.type#14}; it is included here for reference.

\Bquote{
The type of \tt{N} in the type \tt{T[N] is \tt{std::size_t}}.

\example{
\codeblock(text, borders=false){
template<typename T> struct S;
template<typename T, T n> struct S<int[n]> {
  using Q = T;
};

using V = decltype(sizeof 0);
using V = S<int[42]>::Q;        // \serif{OK; \tt{T} was deduced as \tt{std::size_t} from the type \tt{int[42]}}
}
}
}

Immediately following \eelis{temp.deduct.type#14}, insert a new paragraph:

\Bins{
The type of \tcode{N} in the type \tcode{_BitInt(N)} is \tcode{std::size_t}.

\example{
\itemdecl{
template <typename T, T n> void f(_BitInt(n));

f(0wb);                         // \serif{OK; \tcode{T} was deduced as \tcode{std::size_t} from an argument of type \tcode{_BitInt(1)}}
}
}
}

Change \eelis{temp.deduct.type#20} as follows:

\Bdiff{
If \tcode{P} has a form that contains \tcode{<i>},
and if the type of \tcode{i} differs from the type of the corresponding template parameter
of the template named by the enclosing \grammarterm{simple-template-id}
or \grammarterm{splice-specialization-specifier}, deduction fails.
If \tcode{P} has a form that contains \tcode{[i]}
\ins{or \tcode{_BitInt(i)}},
and if the type of \tcode{i} is not an integral type, deduction fails.
If \tcode{P} has a form that includes \tcode{noexcept(i)}
and the type of \tcode{i} is not \tcode{bool}, deduction fails.
}

\h4(show-number=false){[cpp.predefined]}

Add a feature-test macro to the table in \eelis{cpp.predefined} as follows:

\Bins{
\itemdecl{
__cpp_bit_int 20XXXXL
}
}

\h4(show-number=false){[diff.lex]}

\editnote{
See \ref(#bit-int-1).
}

In \eelis{diff.lex},
add a new entry:

\Bins{
\b{Affected subclause}:
\eelis{lex.icon}\br
\b{Change}:
The type of \tcode{0wb} is changed from \tcode{_BitInt(2)} to \tcode{_BitInt(1)}.\br
\b{Rationale}:
It is expected that a future C standard makes the same change,
as part of making \tcode{_BitInt(1)} a valid type.\br
\b{Effect on the original feature}:
Change to semantics of well-defined feature.\br
\b{Difficulty of converting}:
Usually, no changes are required
because the type of \tcode{0wb} is inconsequential.\br
\b{How widely used}:
Seldom.
}

\:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

\h3{Library}

\h4(show-number=false){[version.syn]}

Add the following feature-test macro to \eelis{version.syn}:

\Bins{
\itemdecl{
#define __cpp_lib_bit_int                           20XXXXL
}
}

\h4(show-number=false){[cstdint.syn]}

In \eelis{cstdint.syn},
update the header synopsis as follows:

\Bdiff{
\itemdecl{
namespace std {
  \etc

  using uintmax_t = \exposid{unsigned integer type};
  using uintptr_t = \exposid{unsigned integer type}; // \serif{optional}

\ins{  template<size_t N>
    using bit_int = _BitInt(N);
  template<size_t N>
    using bit_uint = unsigned _BitInt(N);}
}
}
}

Change \eelis{cstdint.syn#2} as follows:

\Bdiff{
The header defines all types and macros the same as the C standard library header \tt{<stdint.h>}.
\ins{None of the aliases name a bit-precise integer type.}
The types denoted by \tcode{intmax_t} and \tcode{uintmax_t}
are not required to be able to represent all values of
\ins{bit-precise integer types or of}
extended integer types wider than
\tcode{\ins_quotes{long long \ins{int}}} and
\tcode{\ins_quotes{unsigned long long \ins{int}}},
respectively.
}

Change \eelis{cstdint.syn#3} as follows:

\Bdiff{
All types that use the placeholder \placeholder{N}
are optional when \placeholder{N}
is not \tcode{8}, \tcode{16}, \tcode{32}, or \tcode{64}.
The exact-width types
\tcode{int\placeholdernc{N}_t} and \tcode{uint\placeholdernc{N}_t}
for \placeholder{N} = \tcode{8}, \tcode{16}, \tcode{32}, and \tcode{64}
are also optional;
however, if an implementation defines integer types
\ins{other than bit-precise integer types}
with the corresponding width and no padding bits,
it defines the corresponding \grammarterm{typedef-name}s.
Each of the macros listed in this subclause
is defined if and only if
the implementation defines the corresponding \grammarterm{typedef-name}.
\br\note{
The macros \tcode{INT\placeholdernc{N}_C} and \tcode{UINT\placeholdernc{N}_C}
correspond to the \grammarterm{typedef-name}s
\tcode{int_least\placeholdernc{N}_t} and \tcode{uint_least\placeholdernc{N}_t},
respectively.
}
}

\h4(show-number=false){[climits.syn]}

In \eelis{climits.syn},
add a new line below the definition of \tcode{\hl(macro){ULLONG_WIDTH}}:

\Bins{
\itemdecl{
#define BITINT_MAXWIDTH \exposid{see below}
}
}

Change the synopsis in \eelis{climits.syn#1} as follows:

\Bdiff{
The header \tt{<climits>} defines all macros
the same as the C standard library header \tt{limits.h}\del{,
except that it does not define the macro \tcode{BITINT_MAXWIDTH}}.
}

\h4(show-number=false){[meta.trans.sign]}

\editnote{
See \ref(#make_signed-and-make_unsigned).
}

Change table \eelis{tab:meta.trans.sign} as follows:

\Bdiff{
\table(class=wording){
  \tr{
    \th{Template}
    \th{Comments}
  }
  \tr{
    \td{
      \itemdecl{
template<class T>
struct make_signed;
}
    }
    \td{
      \ins{Specializations have an alias member \tcode{type} determined as follows:}
      \ul{
        \li{
          If \tcode{T} is a \del{(possibly cv-qualified)}
          signed integer type\iref{basic.fundamental} \del{then the member typedef} \ins{,}
          \tcode{type} denotes \tcode{T} \del{;} \ins{.}
        }
        \li{
          \del{otherwise} \ins{Otherwise}, if \tcode{T} is
          \del{a (possibly cv-qualified)}
          \ins{an} unsigned integer type \del{then} \ins{,}
          \tcode{type} denotes the corresponding signed integer type
          \del{, with the same cv-qualifiers as \tcode{T};} \ins{.}
        }
        \li{\ins{
          Otherwise, if \tcode{T}'s underlying type \tcode{U}
          is a bit-precise signed integer type,
          \tcode{type} denotes \tcode{U}.
        }}
        \li{\ins{
          Otherwise, if \tcode{T}'s underlying type \tcode{U}
          is a bit-precise unsigned integer type,
          \tcode{type} denotes the corresponding signed integer type of \tcode{U}.
        }}
        \li{
          \del{otherwise} \ins{Otherwise},
          \ins{if \tcode{T} is cv-unqualified,}
          \tcode{type} denotes the \ins{standard or extended} signed integer type
          with smallest rank\iref{conv.rank}
          for which \tcode{sizeof(T)} \del{==} \ins{equals} \tcode{sizeof(type)}
          \del{, with the same cv-qualifiers as \tcode{T}}.
        }
        \li{\ins{
          Otherwise, \tcode{T} is a cv-qualified type.
          \tcode{type} denotes the type determined by applying the rules above
          to \tcode{remove_cv_t<T>},
          with the same cv-qualifiers as \tcode{T}.
        }}
      }
      \mandates
      \tcode{T} is an integral or enumeration type other than \tcode{\cv bool}.
    }
  }
  \tr{
    \td{
      \itemdecl{
template<class T>
struct make_unsigned;
}
    }
    \td{
      \ins{Specializations have an alias member \tcode{type} determined as follows:}
      \ul{
        \li{
          If \tcode{T} is a \del{(possibly cv-qualified)}
          unsigned integer type\iref{basic.fundamental} \del{then the member typedef} \ins{,}
          \tcode{type} denotes \tcode{T} \del{;} \ins{.}
        }
        \li{
          \del{otherwise} \ins{Otherwise}, if \tcode{T} is a
          \del{(possibly cv-qualified)}
          signed integer type \del{then} \ins{,}
          \tcode{type} denotes the corresponding unsigned integer type
          \del{, with the same cv-qualifiers as \tcode{T};} \ins{.}
        }
        \li{\ins{
          Otherwise, if \tcode{T}'s underlying type \tcode{U}
          is a bit-precise unsigned integer type,
          \tcode{type} denotes \tcode{U}.
        }}
        \li{\ins{
          Otherwise, if \tcode{T}'s underlying type \tcode{U}
          is a bit-precise signed integer type,
          \tcode{type} denotes the corresponding unsigned integer type of \tcode{U}.
        }}
        \li{
          \del{otherwise} \ins{Otherwise},
          \ins{if \tcode{T} is cv-unqualified,}
          \tcode{type} denotes the \ins{standard or extended} unsigned integer type
          with smallest rank\iref{conv.rank}
          for which \tcode{sizeof(T)} \del{==} \ins{equals} \tcode{sizeof(type)}
          \del{, with the same cv-qualifiers as \tcode{T}}.
        }
        \li{\ins{
          Otherwise, \tcode{T} is a cv-qualified type.
          \tcode{type} denotes the type determined by applying the rules above
          to \tcode{remove_cv_t<T>},
          with the same cv-qualifiers as \tcode{T}.
        }}
      }
      \mandates
      \tcode{T} is an integral or enumeration type other than \tcode{\cv bool}.
    }
  }
}
}

\h4(show-number=false){[stdbit.h.syn]}

Change \eelis{stdbit.h.syn#2} as follows:

\Bdiff{
\mandates
\tcode{T} is \del{an unsigned integer type}
\ul{
  \li{\ins{a standard unsigned integer type,}}
  \li{\ins{an extended unsigned integer type, or}}
  \li{
    \ins{a bit-precise unsigned integer type whose width matches
    a standard or extended integer type}.
  }
}
}

\h4(show-number=false){[range.iota.view]}

\editnote{See \ref(#preventing-iota-view-abi-break).}

Change \eelis{range.iota.view#1} as follows:

\Bdiff{
Let \tcode{\exposid{IOTA-DIFF-T}(W)} be defined as follows:

\ul{
  \li{
    If \tcode{W} is not an integral type,
    or if it is an integral type and \tcode{sizeof(iter_difference_t<W>)} is
    greater than \tcode{sizeof(W)},
    then \tcode{\exposid{IOTA-DIFF-T}(W)} denotes \tcode{iter_difference_t<W>}.
  }
  \li{
    Otherwise, \tcode{\exposid{IOTA-DIFF-T}(W)}
    is a \ins{standard} signed integer type of width greater than the width of \tcode{W}
    if such a type exists.
  }
  \li{
    Otherwise, \tcode{\exposid{IOTA-DIFF-T}(W)}
    is an unspecified signed-integer-like\iref{iterator.concept.winc} type
    of width not less than the width of \tcode{W}.
  }
}
}

\h4(show-number=false){[alg.foreach]}

Change \eelis{alg.foreach#lib:for_each_n} as follows:

\Bdiff{
\itemdecl{
template<class InputIterator, class Size, class Function>
  constexpr InputIterator for_each_n(InputIterator first, Size n, Function f);
}

\itemdescr{
\mandates
The type \tcode{Size} is convertible to an integral type
\ins{other than a bit-precise integer type}
(\eelis{conv.integral}, \eelis{class.conv}).

\etc
}

\itemdecl{
template<class ExecutionPolicy, class ForwardIterator, class Size, class Function>
  ForwardIterator for_each_n(ExecutionPolicy&& exec, ForwardIterator first, Size n,
                             Function f);
}

\itemdescr{
\mandates
The type \tcode{Size} is convertible to an integral type
\ins{other than a bit-precise integer type}
(\eelis{conv.integral}, \eelis{class.conv}).

\etc
}
}

\editnote{
Implementing this requirement for bit-precise integer types is generally impossible,
barring compiler magic.
The libc++ implementation is done by calling an overload in the set:

\cppblock{
int __convert_to_integral(int __val) { return __val; }
unsigned __convert_to_integral(unsigned __val) { return __val; }
}

It is not reasonable to expect millions of additional overloads,
and a template that can handle bit-precise integers in bulk could not interoperate with
user-defined conversion function templates.
}

\h4(show-number=false){[alg.search]}

Change \eelis{alg.search#5} as follows:

\Bdiff{
\mandates
The type \tcode{Size} is convertible to an integral type
\ins{other than a bit-precise integer type}
(\eelis{conv.integral}, \eelis{class.conv}).
}

\h4(show-number=false){[alg.copy]}

Change \eelis{alg.copy#15} as follows:

\Bdiff{
\mandates
The type \tcode{Size} is convertible to an integral type
\ins{other than a bit-precise integer type}
(\eelis{conv.integral}, \eelis{class.conv}).
}

\h4(show-number=false){[alg.fill]}

Change \eelis{alg.fill#2} as follows:

\Bdiff{
\mandates
The expression \tcode{value} is writable\iref{iterator.requirements.general}
to the output iterator.
The type \tcode{Size} is convertible to an integral type
\ins{other than a bit-precise integer type}
(\eelis{conv.integral}, \eelis{class.conv}).
}

\h4(show-number=false){[alg.generate]}

Change \eelis{alg.generate#2} as follows:

\Bdiff{
\mandates
\tcode{Size} is convertible to an integral type
\ins{other than a bit-precise integer type}
(\eelis{conv.integral}, \eelis{class.conv}).
}

\h4(show-number=false){[charconv.syn]}

\editnote{
See \ref(#printing-support).

As explained in that section,
it would be a breaking change to turn the existing overloads into function templates.

The removal of \del{cv-unqualified} below is not an accident:
signed and unsigned integer types do not include any cv-qualified types.
}

Change \eelis{charconv.syn#1} as follows:

\Bdiff{
When a function is specified with a type placeholder of \tcode{\exposid{integer-type}},
the implementation provides overloads for \tcode{char}
and all \del{cv-unqualified signed and unsigned integer types}
\ins{standard and extended integer types}
in lieu of \tcode{\exposid{integer-type}}.
When a function is specified with a type placeholder of \tcode{\exposid{floating-point-type}},
the implementation provides overloads for all
cv-unqualified floating-point types\iref{basic.fundamental}
in lieu of \tcode{\exposid{floating-point-type}}.

\itemdecl{
namespace std {
  // \serif{floating-point format for primitive numerical conversion}
  enum class chars_format {
    scientific = unspecified,
    fixed = unspecified,
    hex = unspecified,
    general = fixed | scientific
  };

  // \serif{[charconv.to.chars], primitive numerical output conversion}
  struct to_chars_result {                                              // \serif{freestanding}
    char* ptr;
    errc ec;
    friend bool operator==(const to_chars_result&, const to_chars_result&) = default;
    constexpr explicit operator bool() const noexcept { return ec == errc{}; }
  };

  constexpr to_chars_result to_chars(char* first, char* last,           // \serif{freestanding}
                                     \exposid{integer-type} value, int base = 10);
\ins{  template<class T>
    constexpr to_chars_result to_chars(char* first, char* last,         // \serif{freestanding}
                                       T value, int base = 10);}
  to_chars_result to_chars(char* first, char* last,                     // \serif{freestanding}
                           bool value, int base = 10) = delete;

  to_chars_result to_chars(char* first, char* last,                     // \serif{freestanding-deleted}
                           \exposid{floating-point-type} value);
  to_chars_result to_chars(char* first, char* last,                     // \serif{freestanding-deleted}
                           \exposid{floating-point-type} value, chars_format fmt);
  to_chars_result to_chars(char* first, char* last,                     // \serif{freestanding-deleted}
                           \exposid{floating-point-type} value, chars_format fmt, int precision);

  // \serif{[charconv.from.chars], primitive numerical input conversion}
  struct from_chars_result {                                            // \serif{freestanding}
    const char* ptr;
    errc ec;
    friend bool operator==(const from_chars_result&, const from_chars_result&) = default;
    constexpr explicit operator bool() const noexcept { return ec == errc{}; }
  };

  constexpr from_chars_result from_chars(const char* first, const char* last,   // \serif{freestanding}
                                         \exposid{integer-type}& value, int base = 10);
\ins{  template<class T>
    constexpr from_chars_result from_chars(char* first, char* last,         // \serif{freestanding}
                                           T& value, int base = 10);}

  from_chars_result from_chars(const char* first, const char* last,     // \serif{freestanding-deleted}
                               \exposid{floating-point-type}& value,
                               chars_format fmt = chars_format::general);
}
}
}

\h4(show-number=false){[charconv.to.chars]}

Change \eelis{charconv.to.chars#lib:to_chars} as follows:

\Bdiff{
\itemdecl{
constexpr to_chars_result to_chars(char* first, char* last, \exposid{integer-type} value, int base = 10);
\ins{template<class T>
  constexpr to_chars_result to_chars(char* first, char* last, T value, int base = 10);}
}

\itemdescr{
\ins{\constraints
\tcode{T} is a bit-precise integer type.}

\expects
\tcode{base} has a value between 2 and 36 (inclusive).

\effects
The value of \tcode{value} is converted to a string of digits
in the given base (with no redundant leading zeroes).
Digits in the range 10..35 (inclusive)
are represented as lowercase characters \tcode{a}..\tcode{z}.
If \tcode{value} is less than zero, the representation starts with \tcode{'-'}.

\throws
Nothing.
}
}

\h4(show-number=false){[charconv.from.chars]}

Change \eelis{charconv.from.chars#lib:from_chars} as follows:

\Bdiff{
\itemdecl{
constexpr from_chars_result from_chars(const char* first, const char* last,
                                       \exposid{integer-type}& value, int base = 10);
\ins{template<class T>
  constexpr from_chars_result from_chars(const char* first, const char* last,
                                         T& value, int base = 10);}
}

\itemdescr{
\ins{\constraints
\tcode{T} is a bit-precise integer type.}

\expects
\tcode{base} has a value between 2 and 36 (inclusive).

\effects
The pattern is the expected form of the subject sequence
in the \tcode{"C"} locale for the given nonzero base,
as described for \tcode{strtol},
except that no \tcode{"0x"} or \tcode{"0X"} prefix shall appear if the value of base is 16,
and except that \tcode{'-'} is the only sign that may appear,
and only if value has a signed type.

\throws
Nothing.
}
}

\h4(show-number=false){[string.syn]}

\editnote{
See \ref(#printing-support).
}

Change \eelis{string.syn} as follows:

\Bdiff{
\itemdecl{
namespace std {
  \etc

  string to_string(int val);
  string to_string(unsigned val);
  string to_string(long val);
  string to_string(unsigned long val);
  string to_string(long long val);
  string to_string(unsigned long long val);
  string to_string(float val);
  string to_string(double val);
  string to_string(long double val);
\ins{  template<class T> string to_string(T val);}

  \etc

  wstring to_wstring(int val);
  wstring to_wstring(unsigned val);
  wstring to_wstring(long val);
  wstring to_wstring(unsigned long val);
  wstring to_wstring(long long val);
  wstring to_wstring(unsigned long long val);
  wstring to_wstring(float val);
  wstring to_wstring(double val);
  wstring to_wstring(long double val);
\ins{  template<class T> wstring to_wstring(T val);}

  \etc
}
}
}

If the existing overloads for integral types
have been made \tcode{constexpr} through \ref(P3438R0) or a subsequent paper,
additionally make the following changes:

\Bdiff{
\itemdecl{
\etc
template<class T> \ins{constexpr} string to_string(T val);
\etc
template<class T> \ins{constexpr} wstring to_wstring(T val);
\etc
}
}

\h4(show-number=false){[string.conversions]}

Change \eelis{string.conversions} as follows:

\Bdiff{
\etc

\itemdecl{
string to_string(int val);
string to_string(unsigned val);
string to_string(long val);
string to_string(unsigned long val);
string to_string(long long val);
string to_string(unsigned long long val);
string to_string(float val);
string to_string(double val);
string to_string(long double val);
\ins{template<class T> string to_string(T val);}
}

\itemdescr{
\ins{\constraints
\tcode{T} is a bit-precise or extended integer type.}

\returns
\tcode{format("{}", val)}.
}

\etc

\itemdecl{
wstring to_wstring(int val);
wstring to_wstring(unsigned val);
wstring to_wstring(long val);
wstring to_wstring(unsigned long val);
wstring to_wstring(long long val);
wstring to_wstring(unsigned long long val);
wstring to_wstring(float val);
wstring to_wstring(double val);
wstring to_wstring(long double val);
\ins{template<class T> wstring to_wstring(T val);}
}

\itemdescr{
\ins{\constraints
\tcode{T} is a bit-precise or extended integer type.}

\returns
\tcode{format(L"{}", val)}.
}

\etc
}

If the existing overloads for integral types
have been made \tcode{constexpr} through \ref(P3438R0) or a subsequent paper,
additionally make the following changes:

\Bdiff{
\itemdecl{
\etc
template<class T> \ins{constexpr} string to_string(T val);
\etc
template<class T> \ins{constexpr} wstring to_wstring(T val);
\etc
}
}

\h4(show-number=false){[cmath.syn]}

\editnote{
\eelis{cmath.syn#3} is deliberately not changed,
meaning that \tcode{bit_int} may be passed to e.g. \tcode{sqrt}.
See \ref(#bitint-cmath).
}

In \eelis{cmath.syn}, change the synopsis as follows:

\Bdiff{
\itemdecl{
constexpr int abs(int j);                                         // \serif{freestanding}
constexpr long int abs(long int j);                               // \serif{freestanding}
constexpr long long int abs(long long int j);                     // \serif{freestanding}
\ins{template<size_t N> constexpr bit_int<N> abs(bit_int<N> j);        // \serif{freestanding}}
constexpr \exposid{floating-point-type} abs(\exposid{floating-point-type} j);         // \serif{freestanding}
}
}

\comment{
Change \eelis{cmath.syn#3} as follows:

\Bdiff{
For each function with at least one parameter of type \tcode{\exposid{floating-point-type}}
other than \tcode{abs},
the implementation also provides additional overloads sufficient to ensure that,
if every argument corresponding to a \tcode{\exposid{floating-point-type}} parameter
has arithmetic type
\ins{other than \cv bit-precise integer type},
then every such argument is effectively cast to the floating-point type
with the greatest floating-point conversion rank
and greatest floating-point conversion subrank among the types of all such arguments,
where arguments of integer type are considered to have
the same floating-point conversion rank as double.
If no such floating-point type with the greatest rank and subrank exists,
then overload resolution does not result in a usable candidate\iref{over.match.general}
from the overloads provided by the implementation.
}
}

\h4(show-number=false){[c.math.abs]}

\editnote{
See \ref(#bitint-abs).
}

Change \eelis{c.math.abs} as follows:

\Bdiff{
\itemdecl{
constexpr int abs(int j);
constexpr long int abs(long int j);
constexpr long long int abs(long long int j);
\ins{template<size_t N> constexpr bit_int<N> abs(bit_int<N> j);}
}

\del{\effects
These functions have the semantics specified in the C standard library for the functions
\tcode{abs}, \tcode{labs}, and \tcode{llabs}, respectively.}

\del{\remarks
If \tcode{abs} is called with an argument of type \tcode{X}
for which \tcode{is_unsigned_v<X>} is \tcode{true}
and if \tcode{X} cannot be converted to \tcode{int} by integral promotion,
the program is ill-formed.
\br\note{
Allowing arguments that can be promoted to \tcode{int} provides compatibility with C.
}}

\ins{\effects
Equivalent to \tcode{j >= 0 ? j : -j}.
\br\note{The behavior is undefined if \tcode{j}
has the lowest possible integer value of its type\iref{expr.pre}.}}
}

\editnote{
Specifying the undefined behavior as a \i{Preconditions} specification
would be worse because it may cause library UB during constant evaluation.

The \i{Effects} specification needs to be altered because \tcode{abs}
for bit-precise integers is a novel invention with no C counterpart.
It also seems like unnecessary indirection to refer to another language standard
for a single expression.

The \i{Remarks} specification is removed
because it is a usage tutorial and history lesson;
it does not say anything about what \tcode{abs} does.
The specification is also factually wrong.
Just because an attempt is made to call \tcode{abs(0u)} and the overloads above
don't handle it,
doesn't mean that the user doesn't have their own
\tcode{abs(unsigned)} overload.
In that event, the program is not ill-formed;
overload resolution simply doesn't select one of these functions.
}

\h4(show-number=false){[simd.general]}

Change \eelis{simd.general#2} as follows:

\Bdiff{
The set of \dfn{vectorizable types} comprises

\ul{
  \li{
    all standard integer types,
    character types,
    and the types \tcode{float} and \tcode{double}\iref{basic.fundamental};
  }
  \li{
    \ins{any bit-precise integer type
    whose width matches a standard integer type;}
  }
  \li{
    \tcode{std::float16_t}, \tcode{std::float32_t}, and \tcode{std::float64_t}
    if defined\iref{basic.extended.fp}; and
  }
  \li{
    \tcode{complex<T>} where \tcode{T} is a vectorizable floating-point type.
  }
}
}

\h4(show-number=false){[numerics.c.ckdint]}

Change \eelis{numerics.c.ckdint} as follows:

\Bdiff{
\itemdecl{
template<class type1, class type2, class type3>
  bool ckd_add(type1* result, type2 a, type3 b);
template<class type1, class type2, class type3>
  bool ckd_sub(type1* result, type2 a, type3 b);
template<class type1, class type2, class type3>
  bool ckd_mul(type1* result, type2 a, type3 b);
}

\mandates
\ins{\tcode{type1} is a signed or unsigned integer type.}
Each of the types \del{\tcode{type1},}
\tcode{type2}\del{,} and \tcode{type3}
is a \del{cv-unqualified} signed or unsigned integer type
\ins{other than a bit-precise integer type}.

\remarks
Each function template has the same semantics
as the corresponding type-generic macro with the same name
specified in \IsoC, 7.20.
}

\editnote{
This matches the restrictions in \ref(N3550), 7.20 "Checked Integer Arithmetic".
"cv-unqualified" is struck because it is redundant.
}

\h4(show-number=false){[atomics.ref.int]}

Do \u{not} change \eelis{atomics.ref.int#1};
it is provided here for reference:

\Bquote{
There are specializations of the \tcode{atomic_ref} class template
for all integral types except \cv \tcode{bool}.
For each such type \tcode{\exposid{integral-type}},
the specialization \tcode{atomic_ref<\exposid{integral-type}>}
provides additional atomic operations
appropriate to integral types.
}

\h4(show-number=false){[atomics.types.int]}

Change \eelis{atomics.types.int#1} as follows:

\Bdiff{
There are specializations of the \tcode{atomic} class template for
\del{the integral types
\tcode{char},
\tcode{signed char},
\tcode{unsigned char},
\tcode{short},
\tcode{unsigned short},
\tcode{int},
\tcode{unsigned int},
\tcode{long},
\tcode{unsigned long},
\tcode{long long},
\tcode{unsigned long long},
\tcode{char8_t},
\tcode{char16_t},
\tcode{char32_t},
\tcode{wchar_t},}
\ins{standard integer types\iref{basic.fundamental},
bit-precise integer types,
character types,}
and any other types needed by the typedefs
in the header \tt{<cstdint>}\iref{cstdint.syn}.
For each such type \tcode{\exposid{integral-type}},
the specialization \tcode{atomic<\exposid{integral-type}>}
provides additional atomic operations
appropriate to integral types.

\note{
The specialization \tcode{atomic<bool>}
uses the primary template\iref{atomics.types.generic}.
}
}

\h2{Acknowledgements}

I thank Jens Maurer and Christof Meerwald
for reviewing and correcting the proposal's wording.

I thank Erich Keane and other LLVM contributors
for implementing most of the proposed core changes in Clang's C++ frontend,
giving this paper years worth of implementation experience in a major compiler
without any effort by the author.

I thank
Erich Keane,
Bill Seymour,
Howard Hinnant,
JeanHeyd Meneide,
Lénárd Szolnoki,
Brian Bi,
Peter Dimov,
Aaron Ballman,
Pete Becker,
Jens Maurer,
Matthias Kretz,
Jonathan Wakely,
Jeff Garland,
Ville Voutilainen,
Peter Dimov,
Luigi Ghiron,
and \em{many} others for providing early feedback on this paper,
prior papers such as \ref(P3639R0), and
the discussion surrounding bit-precise integers as a whole.
The paper would not be where it is today without \em{hundreds}
of messages worth of valuable feedback.

\h2{References}

\bib(
  id = N1692,
  title = A Proposal to add the Infinite Precision Integer to the C++ Standard Library,
  date = 2004-07-01,
  author = M.J. Kronenburg,
  link = https://wg21.link/n1692,
  long-link = https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2004/n1692.pdf,
)\
\bib(
  id = N1744,
  title = Big Integer Library Proposal for C++0x,
  date = 2005-01-13,
  author = Michiel Salters,
  link = https://wg21.link/n1744,
  long-link = https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2005/n1744.pdf,
)\
\bib(
  id = N4038,
  title = Proposal for Unbounded-Precision Integer Types,
  date = 2014-05-23,
  author = Pete Becker,
  link = https://wg21.link/n4038,
  long-link = https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4038.html,
)\
\bib(
  id = N5014,
  title = Working Draft\, Programming Languages — C++,
  date = 2025-08-05,
  author = Thomas Köppe,
  link = https://wg21.link/n5014,
  long-link = https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/n5014.pdf,
)\
\bib(
  id = P3140R0,
  title = std::int_least128_t,
  date = 2025-02-11,
  author = Jan Schultke,
  link = https://wg21.link/p3140r0,
  long-link = https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2024/p3140r0.html,
)\
\bib(
  id = P3161R4,
  title = Unified integer overflow arithmetic,
  date = 2025-03-24,
  author = Tiago Freire,
  link = https://wg21.link/p3161r4,
  long-link = https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3161r4.html,
)\
\bib(
  id = P3639R0,
  title = The _BitInt Debate,
  date = 2025-02-20,
  author = Jan Schultke,
  link = https://wg21.link/p3639r0,
  long-link = https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3639r0.html,
)\
\bib(
  id = P3312R1,
  title = Overload Set Types,
  date = 2025-04-16,
  author = Bengt Gustafsson,
  link = https://wg21.link/p3312r1,
  long-link = https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3312r1.pdf,
)\
\bib(
  id = P3438R0,
  title = Make integral overloads of std::to_string constexpr,
  date = 2024-10-13,
  author = Andreas Fertig,
  link = https://wg21.link/p3438r0,
  long-link = https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2024/p3438r0.pdf,
)\
\bib(
  id = N2763,
  title = Adding a Fundamental Type for N-bit integers,
  date = 2021-06-21,
  author = Aaron Ballman\, Melanie Blower\, Tommy Hoffner\, Erich Keane,
  link = https://open-std.org/JTC1/SC22/WG14/www/docs/n2763.pdf,
)\
\bib(
  id = N2775,
  title = Literal suffixes for bit-precise integers,
  date = 2021-07-13,
  author = Aaron Ballman\, Melanie Blower,
  link = https://open-std.org/JTC1/SC22/WG14/www/docs/n2775.pdf,
)\
\bib(
  id = N3550,
  title = ISO/IEC 9899:202y (en) — N3550 working draft,
  date = 2025-05-04,
  author = JeanHeyd Meneide,
  link = https://www.open-std.org/jtc1/sc22/wg14/www/docs/n3550.pdf,
)\
\bib(
  id = N3699,
  title = Integer Sets\, v3,
  date = 2025-09-02,
  author = Robert C. Seacord,
  link = https://www.open-std.org/jtc1/sc22/wg14/www/docs/n3699.pdf,
)\
\bib(
  id = N3705,
  title = bit-precise enum,
  date = 2025-09-05,
  author = Phillip Klaus Krause,
  link = https://www.open-std.org/jtc1/sc22/wg14/www/docs/n3705.htm,
)\

\make_bib
